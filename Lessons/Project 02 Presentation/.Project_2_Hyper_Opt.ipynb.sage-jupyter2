{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":84156416},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"4e8c02","input":"","pos":83,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"d57dbc","input":"# EXECUTE FIRST\n\n# computational imports\nimport numpy as np\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, KFold, RandomizedSearchCV\nimport xgboost as xgb\nfrom scipy.stats import uniform, randint\nfrom GPyOpt.methods import BayesianOptimization\nfrom tpot import TPOTRegressor\nfrom pprint import pprint\n\n# plotting imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\n# for reading files from urls\nimport urllib.request\n# display imports\nfrom IPython.display import display, IFrame\nfrom IPython.core.display import HTML\n\n# import notebook styling for tables and width etc.\nresponse = urllib.request.urlopen('https://raw.githubusercontent.com/DataScienceUWL/DS775v2/master/ds755.css')\nHTML(response.read().decode(\"utf-8\"));\n\n# import warnings\nimport warnings","metadata":{"code_folding":[0]},"pos":0,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"4764ab","input":"# from sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor(random_state=0)\nrf_model.fit(X_train,y_train)\n\nmy_regression_results(rf_model)","output":{"0":{"name":"stdout","output_type":"stream","text":"Model r-squared score from test data: 0.5368\nMean squared error on test data: 2918.49\nRoot mean squared error on test data: 54.02\n"},"1":{"data":{"image/png":"8f599b190b8b55b230d68ba88a21d33cf7ef5143","text/plain":"<Figure size 648x432 with 1 Axes>"},"exec_count":10,"metadata":{"image/png":{"height":370,"width":560}},"output_type":"execute_result"}},"pos":29,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"1a857a","input":"# import xgboost as xgb\n\nxgbr_model = xgb.XGBRegressor(objective ='reg:squarederror')\nxgbr_model.fit(X_train,y_train)\n\nmy_regression_results(xgbr_model)","output":{"0":{"name":"stdout","output_type":"stream","text":"Model r-squared score from test data: 0.5143\nMean squared error on test data: 3060.30\nRoot mean squared error on test data: 55.32\n"},"1":{"data":{"image/png":"5d3f7c055c2e5d3f26a982506c396bc4e3eb3bed","text/plain":"<Figure size 648x432 with 1 Axes>"},"exec_count":11,"metadata":{"image/png":{"height":370,"width":560}},"output_type":"execute_result"}},"pos":33,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"255469","input":"# from sklearn.model_selection import cross_val_score, KFold\nscores = cross_val_score(xgbr_model, X=X_train, y=y_train, cv = 3)\nprint(f\"The average score across the folds is {scores.mean():.4f}\")","output":{"0":{"name":"stdout","output_type":"stream","text":"The average score across the folds is 0.3733\n"}},"pos":37,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"df05cb","input":"# from pprint import pprint\npprint(xgbr_model.get_xgb_params())","output":{"0":{"name":"stdout","output_type":"stream","text":"{'base_score': 0.5,\n 'booster': 'gbtree',\n 'colsample_bylevel': 1,\n 'colsample_bynode': 1,\n 'colsample_bytree': 1,\n 'gamma': 0,\n 'importance_type': 'gain',\n 'learning_rate': 0.1,\n 'max_delta_step': 0,\n 'max_depth': 3,\n 'min_child_weight': 1,\n 'missing': None,\n 'n_estimators': 100,\n 'nthread': 1,\n 'objective': 'reg:squarederror',\n 'reg_alpha': 0,\n 'reg_lambda': 1,\n 'scale_pos_weight': 1,\n 'seed': 0,\n 'subsample': 1,\n 'verbosity': 1}\n"}},"pos":41,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"5ccb8d","input":"# run GridSearchCV with our xgbr_model to find better hyperparameters\n# from sklearn.model_selection import GridSearchCV\n\n# define the grid\nparams = {\n    \"learning_rate\": [0.01, 0.1],\n    \"max_depth\": [2, 4, 6],\n    \"n_estimators\": [10, 100],\n    \"subsample\": [0.8, 1],\n    \"min_child_weight\": [1, 3],\n    \"reg_lambda\": [1, 3],\n    \"reg_alpha:\": [1, 3]\n}\n\n# setup the grid search\ngrid_search = GridSearchCV(xgbr_model,\n                           param_grid=params,\n                           cv=5,\n                           verbose=1,\n                           n_jobs=1,\n                           return_train_score=True)\n\ngrid_search.fit(X_train, y_train)","output":{"0":{"name":"stdout","output_type":"stream","text":"Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"},"1":{"name":"stderr","output_type":"stream","text":"[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"},"2":{"name":"stderr","output_type":"stream","text":"[Parallel(n_jobs=1)]: Done 960 out of 960 | elapsed:   25.7s finished\n"},"3":{"data":{"text/plain":"GridSearchCV(cv=5, estimator=XGBRegressor(objective='reg:squarederror'),\n             n_jobs=1,\n             param_grid={'learning_rate': [0.01, 0.1], 'max_depth': [2, 4, 6],\n                         'min_child_weight': [1, 3], 'n_estimators': [10, 100],\n                         'reg_alpha:': [1, 3], 'reg_lambda': [1, 3],\n                         'subsample': [0.8, 1]},\n             return_train_score=True, verbose=1)"},"exec_count":14,"output_type":"execute_result"}},"pos":45,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"69dc7b","input":"grid_search.best_params_","output":{"0":{"data":{"text/plain":"{'learning_rate': 0.1,\n 'max_depth': 2,\n 'min_child_weight': 3,\n 'n_estimators': 100,\n 'reg_alpha:': 1,\n 'reg_lambda': 3,\n 'subsample': 0.8}"},"exec_count":15,"output_type":"execute_result"}},"pos":47,"type":"cell"}
{"cell_type":"code","exec_count":16,"id":"24b562","input":"my_regression_results(grid_search)","output":{"0":{"name":"stdout","output_type":"stream","text":"Model r-squared score from test data: 0.6157\nMean squared error on test data: 2421.23\nRoot mean squared error on test data: 49.21\n"},"1":{"data":{"image/png":"d34b9a1b8d4a4f96af1ed9ce7ebec10dd16c4bd3","text/plain":"<Figure size 648x432 with 1 Axes>"},"exec_count":16,"metadata":{"image/png":{"height":370,"width":560}},"output_type":"execute_result"}},"pos":49,"type":"cell"}
{"cell_type":"code","exec_count":17,"id":"8a38d6","input":"# from sklearn.model_selection import RandomizedSearchCV\n# from scipy.stats import uniform, randint\n\nparams = {\n    \"learning_rate\": [0.001, 0.01, 0.1, 0.5, 1.],\n    \"max_depth\": randint(1, 10),\n    \"n_estimators\": randint(10, 100),\n    \"subsample\": uniform(0.05, 0.95),  # so uniform on [.05,.05+.95] = [.05,1.]\n    \"min_child_weight\": randint(1, 20),\n    \"reg_alpha\": uniform(0, 5),\n    \"reg_lambda\": uniform(0, 5)\n}\n\nrandom_search = RandomizedSearchCV(\n    xgbr_model,\n    param_distributions=params,\n    random_state=8675309,\n    n_iter=25,\n    cv=5,\n    verbose=1,\n    n_jobs=1,\n    return_train_score=True)\n\nrandom_search.fit(X_train, y_train)","output":{"0":{"name":"stdout","output_type":"stream","text":"Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"},"1":{"name":"stderr","output_type":"stream","text":"[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"},"2":{"name":"stderr","output_type":"stream","text":"[Parallel(n_jobs=1)]: Done 125 out of 125 | elapsed:    2.5s finished\n"},"3":{"data":{"text/plain":"RandomizedSearchCV(cv=5, estimator=XGBRegressor(objective='reg:squarederror'),\n                   n_iter=25, n_jobs=1,\n                   param_distributions={'learning_rate': [0.001, 0.01, 0.1, 0.5,\n                                                          1.0],\n                                        'max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f349d47ac18>,\n                                        'min_child_weight': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f349d483588>,\n                                        'n_estimators': <scipy.sta...istn_infrastructure.rv_frozen object at 0x7f349d47ada0>,\n                                        'reg_alpha': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f349d483240>,\n                                        'reg_lambda': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f349d4838d0>,\n                                        'subsample': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f349d47af60>},\n                   random_state=8675309, return_train_score=True, verbose=1)"},"exec_count":17,"output_type":"execute_result"}},"pos":53,"type":"cell"}
{"cell_type":"code","exec_count":18,"id":"807cfe","input":"random_search.best_params_","output":{"0":{"data":{"text/plain":"{'learning_rate': 0.1,\n 'max_depth': 7,\n 'min_child_weight': 7,\n 'n_estimators': 82,\n 'reg_alpha': 0.30321968913636976,\n 'reg_lambda': 2.1414785142199557,\n 'subsample': 0.1011386086671309}"},"exec_count":18,"output_type":"execute_result"}},"pos":55,"type":"cell"}
{"cell_type":"code","exec_count":19,"id":"9b85cb","input":"my_regression_results(random_search)","output":{"0":{"name":"stdout","output_type":"stream","text":"Model r-squared score from test data: 0.5547\nMean squared error on test data: 2805.16\nRoot mean squared error on test data: 52.96\n"},"1":{"data":{"image/png":"2f34427e856485fd6e77279d336e7b61b912e3af","text/plain":"<Figure size 648x432 with 1 Axes>"},"exec_count":19,"metadata":{"image/png":{"height":370,"width":560}},"output_type":"execute_result"}},"pos":56,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"072dc3","input":"# imports in first cell of notebook\n# from sklearn.datasets import load_diabetes\ndiabetes = load_diabetes()\nprint(diabetes.DESCR)","output":{"0":{"name":"stdout","output_type":"stream","text":".. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, T-Cells (a type of white blood cells)\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, thyroid stimulating hormone\n      - s5      ltg, lamotrigine\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":20,"id":"0846d1","input":"# unfold to see code\nnp.random.seed(8675309)  # seed courtesy of Tommy Tutone\n# from GPyOpt.methods import BayesianOptimization\n# from sklearn.model_selection import cross_val_score, KFold\n\nhp_bounds = [{\n    'name': 'learning_rate',\n    'type': 'continuous',\n    'domain': (0.001, 1.0)\n}, {\n    'name': 'max_depth',\n    'type': 'discrete',\n    'domain': (1, 10)\n}, {\n    'name': 'n_estimators',\n    'type': 'discrete',\n    'domain': (10, 100)\n}, {\n    'name': 'subsample',\n    'type': 'continuous',\n    'domain': (0.05, 1.0)\n}, {\n    'name': 'min_child_weight',\n    'type': 'discrete',\n    'domain': (1, 20)\n}, {\n    'name': 'reg_alpha',\n    'type': 'continuous',\n    'domain': (0, 5)\n}, {\n    'name': 'reg_lambda',\n    'type': 'continuous',\n    'domain': (0, 5)\n}]\n\n\n# Optimization objective\ndef cv_score(hyp_parameters):\n    hyp_parameters = hyp_parameters[0]\n    xgb_model = xgb.XGBRegressor(objective='reg:squarederror',\n                                 learning_rate=hyp_parameters[0],\n                                 max_depth=int(hyp_parameters[1]),\n                                 n_estimators=int(hyp_parameters[2]),\n                                 subsample=hyp_parameters[3],\n                                 min_child_weight=int(hyp_parameters[4]),\n                                 reg_alpha=hyp_parameters[5],\n                                 reg_lambda=hyp_parameters[6])\n    scores = cross_val_score(xgb_model,\n                             X=X_train,\n                             y=y_train,\n                             cv=KFold(n_splits=5))\n    return np.array(scores.mean())  # return average of 5-fold scores\n\n\noptimizer = BayesianOptimization(f=cv_score,\n                                 domain=hp_bounds,\n                                 model_type='GP',\n                                 acquisition_type='EI',\n                                 acquisition_jitter=0.05,\n                                 exact_feval=True,\n                                 maximize=True,\n                                 verbosity=True)\n\noptimizer.run_optimization(max_iter=20,verbosity=True)","output":{"0":{"name":"stdout","output_type":"stream","text":"num acquisition: 1, time elapsed: 0.33s\n"},"1":{"name":"stdout","output_type":"stream","text":"num acquisition: 2, time elapsed: 0.70s\n"},"10":{"name":"stdout","output_type":"stream","text":"num acquisition: 11, time elapsed: 5.01s\n"},"11":{"name":"stdout","output_type":"stream","text":"num acquisition: 12, time elapsed: 5.54s\n"},"12":{"name":"stdout","output_type":"stream","text":"num acquisition: 13, time elapsed: 6.78s\n"},"13":{"name":"stdout","output_type":"stream","text":"num acquisition: 14, time elapsed: 8.20s\n"},"14":{"name":"stdout","output_type":"stream","text":"num acquisition: 15, time elapsed: 9.37s\n"},"15":{"name":"stdout","output_type":"stream","text":"num acquisition: 16, time elapsed: 10.53s\n"},"16":{"name":"stdout","output_type":"stream","text":"num acquisition: 17, time elapsed: 11.89s\n"},"17":{"name":"stdout","output_type":"stream","text":"num acquisition: 18, time elapsed: 13.00s\n"},"18":{"name":"stdout","output_type":"stream","text":"num acquisition: 19, time elapsed: 14.27s\n"},"19":{"name":"stdout","output_type":"stream","text":"num acquisition: 20, time elapsed: 15.46s\n"},"2":{"name":"stdout","output_type":"stream","text":"num acquisition: 3, time elapsed: 1.03s\n"},"3":{"name":"stdout","output_type":"stream","text":"num acquisition: 4, time elapsed: 1.65s\n"},"4":{"name":"stdout","output_type":"stream","text":"num acquisition: 5, time elapsed: 2.07s\n"},"5":{"name":"stdout","output_type":"stream","text":"num acquisition: 6, time elapsed: 2.54s\n"},"6":{"name":"stdout","output_type":"stream","text":"num acquisition: 7, time elapsed: 2.99s\n"},"7":{"name":"stdout","output_type":"stream","text":"num acquisition: 8, time elapsed: 3.52s\n"},"8":{"name":"stdout","output_type":"stream","text":"num acquisition: 9, time elapsed: 4.02s\n"},"9":{"name":"stdout","output_type":"stream","text":"num acquisition: 10, time elapsed: 4.47s\n"}},"pos":60,"type":"cell"}
{"cell_type":"code","exec_count":21,"id":"46d84f","input":"best_hyp_set = {}\nfor i in range(len(hp_bounds)):\n    if hp_bounds[i]['type'] == 'continuous':\n        best_hyp_set[hp_bounds[i]['name']] = optimizer.x_opt[i]\n    else:\n        best_hyp_set[hp_bounds[i]['name']] = int(optimizer.x_opt[i])\nbest_hyp_set","output":{"0":{"data":{"text/plain":"{'learning_rate': 0.15700765962373503,\n 'max_depth': 1,\n 'n_estimators': 100,\n 'subsample': 0.749270303754013,\n 'min_child_weight': 20,\n 'reg_alpha': 3.0403511390716824,\n 'reg_lambda': 1.842598280636247}"},"exec_count":21,"output_type":"execute_result"}},"pos":62,"type":"cell"}
{"cell_type":"code","exec_count":22,"id":"36b3a7","input":"bayopt_search = xgb.XGBRegressor(objective='reg:squarederror',**best_hyp_set)\nbayopt_search.fit(X_train,y_train)","output":{"0":{"data":{"text/plain":"XGBRegressor(learning_rate=0.15700765962373503, max_depth=1,\n             min_child_weight=20, objective='reg:squarederror',\n             reg_alpha=3.0403511390716824, reg_lambda=1.842598280636247,\n             subsample=0.749270303754013)"},"exec_count":22,"output_type":"execute_result"}},"pos":64,"type":"cell"}
{"cell_type":"code","exec_count":23,"id":"4224ad","input":"my_regression_results(bayopt_search)","output":{"0":{"name":"stdout","output_type":"stream","text":"Model r-squared score from test data: 0.6286\nMean squared error on test data: 2339.78\nRoot mean squared error on test data: 48.37\n"},"1":{"data":{"image/png":"919f7c58247808cae9405bbee58e32ae1a48a402","text/plain":"<Figure size 648x432 with 1 Axes>"},"exec_count":23,"metadata":{"image/png":{"height":370,"width":560}},"output_type":"execute_result"}},"pos":66,"type":"cell"}
{"cell_type":"code","exec_count":24,"id":"6f12fe","input":"# from tpot import TPOTRegressor\n\ntpot_config = {\n    'xgboost.XGBRegressor': {\n        'n_estimators': [100],\n        'max_depth': range(1, 11),\n        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n        'subsample': np.arange(0.05, 1.01, 0.05),\n        'min_child_weight': range(1, 21),\n        'reg_alpha': range(1, 6),\n        'reg_lambda': range(1, 6),\n        'nthread': [1],\n        'objective': ['reg:squarederror']\n    }\n}\n\ntpot = TPOTRegressor(scoring = 'r2',\n                     generations=5,\n                     population_size=20,\n                     verbosity=2,\n                     config_dict=tpot_config,\n                     cv=5,\n                     random_state=8675309)\ntpot.fit(X_train, y_train)\ntpot.export('tpot_XGBregressor.py') # export the model","metadata":{"hidden":true},"output":{"0":{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e597bee74b3944e3a3336223bd504f90","version_major":2,"version_minor":0},"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=120.0, style=ProgressStyle(de…"},"exec_count":24,"output_type":"execute_result"},"1":{"name":"stdout","output_type":"stream","text":"\rGeneration 1 - Current best internal CV score: 0.41473424593500796\n"},"2":{"name":"stdout","output_type":"stream","text":"\rGeneration 2 - Current best internal CV score: 0.41539686865502573\n"},"3":{"name":"stdout","output_type":"stream","text":"\rGeneration 3 - Current best internal CV score: 0.41539686865502573\n"},"4":{"name":"stdout","output_type":"stream","text":"\rGeneration 4 - Current best internal CV score: 0.41539686865502573\n"},"5":{"name":"stdout","output_type":"stream","text":"\rGeneration 5 - Current best internal CV score: 0.4365513495756462\n\nBest pipeline: XGBRegressor(CombineDFs(input_matrix, input_matrix), learning_rate=0.1, max_depth=1, min_child_weight=2, n_estimators=100, nthread=1, objective=reg:squarederror, reg_alpha=3, reg_lambda=2, subsample=0.5)\n"}},"pos":70,"type":"cell"}
{"cell_type":"code","exec_count":25,"id":"6b559c","input":"my_regression_results(tpot)","metadata":{"hidden":true},"output":{"0":{"name":"stdout","output_type":"stream","text":"Model r-squared score from test data: 0.5928\nMean squared error on test data: 2565.19\nRoot mean squared error on test data: 50.65\n"},"1":{"data":{"image/png":"5029c6d99bbe8712369923d6897531a2c99cd15f","text/plain":"<Figure size 648x432 with 1 Axes>"},"exec_count":25,"metadata":{"image/png":{"height":370,"width":560}},"output_type":"execute_result"}},"pos":72,"type":"cell"}
{"cell_type":"code","exec_count":26,"id":"48db88","input":"# from tpot import TPOTRegressor\n\ntpot = TPOTRegressor(scoring = 'r2',\n                     generations=10,\n                     population_size=40,\n                     verbosity=2,\n                     cv=5,\n                     random_state=8675309)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_optimal_pipeline.py')","metadata":{"code_folding":[],"hidden":true},"output":{"0":{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"622c96ba570244bca5de10d424a72240","version_major":2,"version_minor":0},"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=440.0, style=ProgressStyle(de…"},"exec_count":26,"output_type":"execute_result"},"1":{"name":"stdout","output_type":"stream","text":"\rGeneration 1 - Current best internal CV score: 0.4577901475756865\n"},"10":{"name":"stdout","output_type":"stream","text":"\rGeneration 10 - Current best internal CV score: 0.47137076042557907\n\nBest pipeline: RidgeCV(FastICA(KNeighborsRegressor(SGDRegressor(input_matrix, alpha=0.0, eta0=1.0, fit_intercept=True, l1_ratio=0.75, learning_rate=constant, loss=huber, penalty=elasticnet, power_t=1.0), n_neighbors=32, p=1, weights=uniform), tol=0.9))\n"},"11":{"name":"stdout","output_type":"stream","text":"0.5664940041154499\n"},"2":{"name":"stdout","output_type":"stream","text":"\rGeneration 2 - Current best internal CV score: 0.4582920539769801\n"},"3":{"name":"stdout","output_type":"stream","text":"\rGeneration 3 - Current best internal CV score: 0.4622345719978803\n"},"4":{"name":"stdout","output_type":"stream","text":"\rGeneration 4 - Current best internal CV score: 0.4622345719978803\n"},"5":{"name":"stdout","output_type":"stream","text":"\rGeneration 5 - Current best internal CV score: 0.4626616999667756\n"},"6":{"name":"stdout","output_type":"stream","text":"\rGeneration 6 - Current best internal CV score: 0.47137076042557907\n"},"7":{"name":"stdout","output_type":"stream","text":"\rGeneration 7 - Current best internal CV score: 0.47137076042557907\n"},"8":{"name":"stdout","output_type":"stream","text":"\rGeneration 8 - Current best internal CV score: 0.47137076042557907\n"},"9":{"name":"stdout","output_type":"stream","text":"\rGeneration 9 - Current best internal CV score: 0.47137076042557907\n"}},"pos":76,"type":"cell"}
{"cell_type":"code","exec_count":27,"id":"1a4e70","input":"my_regression_results(tpot)","metadata":{"hidden":true},"output":{"0":{"name":"stdout","output_type":"stream","text":"Model r-squared score from test data: 0.5665\nMean squared error on test data: 2731.16\n"},"1":{"name":"stdout","output_type":"stream","text":"Root mean squared error on test data: 52.26\n"},"2":{"data":{"image/png":"7cb981412de9cce3b0c2e344720345c58f26a204","text/plain":"<Figure size 648x432 with 1 Axes>"},"exec_count":27,"metadata":{"image/png":{"height":370,"width":560}},"output_type":"execute_result"}},"pos":77,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"02f0f2","input":"# import numpy as np\nX = np.array(diabetes.data)\ny = np.array(diabetes.target)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)","pos":10,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"b8991d","input":"# Step 1: import the model\n# from sklearn.linear_model import LinearRegression\n\n# Step 2: create a model object (hyperparameters are declared here, we are using default values) \nmodel_lr = LinearRegression()\n\n# Step 3: fit the model by calling its fit() method\nmodel_lr.fit(X_train, y_train)","output":{"0":{"data":{"text/plain":"LinearRegression()"},"exec_count":4,"output_type":"execute_result"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"94c2d7","input":"score_training = model_lr.score(X_train,y_train)\nprint(f\"Model r-squared score from training data {score_training:.4f}\")","output":{"0":{"name":"stdout","output_type":"stream","text":"Model r-squared score from training data 0.5016\n"}},"pos":17,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"f199bb","input":"# Step 4 - assess model quality on test data\nscore_test = model_lr.score(X_test,y_test)\nprint(f\"Model r-squared score from test data: {score_test:.4f}\")","output":{"0":{"name":"stdout","output_type":"stream","text":"Model r-squared score from test data: 0.5676\n"}},"pos":19,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"078ff0","input":"# Step 5 - make predictions\ny_pred = model_lr.predict(X_test)\n\n# The plot is optional, but it gives an idea of the model accuracy, \n# in a perfect model the points would line up along the diagonal (y=x)\n# import matplotlib.pyplot as plt\nplt.figure(figsize=(9,6))\nplt.plot(y_test,y_pred,'k.')\nplt.xlabel('Test Values')\nplt.ylabel('Predicted Values');","output":{"0":{"data":{"image/png":"d080d471b0eb149600dad5db54349566d2799d3b","text/plain":"<Figure size 648x432 with 1 Axes>"},"exec_count":7,"metadata":{"image/png":{"height":370,"width":560}},"output_type":"execute_result"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"036f15","input":"# Step 6: Assess accuracy on test-data.\n\n# from sklearn.metrics import mean_squared_error\nmse = mean_squared_error(y_test,y_pred)\nrmse = np.sqrt(mse)\nprint(f\"Mean squared error on test data: {mse:.2f}\")\nprint(f\"Root mean squared error on test data: {rmse:.2f}\")","output":{"0":{"name":"stdout","output_type":"stream","text":"Mean squared error on test data: 2724.24\nRoot mean squared error on test data: 52.19\n"}},"pos":23,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"898a4b","input":"# Here is all the code in one cell with most of it wrapped into a function for reuse\n\n# from sklearn.linear_model import LinearRegression\nmodel_lr = LinearRegression()\nmodel_lr.fit(X_train,y_train) # this could be inside the function below too\n\ndef my_regression_results(model):\n    score_test = model.score(X_test,y_test)\n    print('Model r-squared score from test data: {:0.4f}'.format(score_test))\n\n    y_pred = model.predict(X_test)\n    # import matplotlib.pyplot as plt\n    plt.figure(figsize=(9,6))\n    plt.plot(y_test,y_pred,'k.')\n    plt.xlabel('Test Values')\n    plt.ylabel('Predicted Values');\n\n    # from sklearn.metrics import mean_squared_error\n    mse = mean_squared_error(y_test,y_pred)\n    rmse = np.sqrt(mse)\n    print('Mean squared error on test data: {:0.2f}'.format(mse))\n    print('Root mean squared error on test data: {:0.2f}'.format(rmse))\n    \nmy_regression_results(model_lr)","output":{"0":{"name":"stdout","output_type":"stream","text":"Model r-squared score from test data: 0.5676\nMean squared error on test data: 2724.24\nRoot mean squared error on test data: 52.19\n"},"1":{"data":{"image/png":"d080d471b0eb149600dad5db54349566d2799d3b","text/plain":"<Figure size 648x432 with 1 Axes>"},"exec_count":9,"metadata":{"image/png":{"height":370,"width":560}},"output_type":"execute_result"}},"pos":25,"type":"cell"}
{"cell_type":"markdown","id":"001b99","input":"This is a nice improvement over the default XGBoost regression model.  Our tuned model now performs better than the linear regression model we saw above.\n\nThe main drawback to grid search is that it can get really expensive if we want to exhaustively search, particularly if the model fits are slow as they can be when large datasets with many predictors are used.","pos":50,"type":"cell"}
{"cell_type":"markdown","id":"0137ff","input":"The random forest model, with default parameters, is not as good as the linear regression model.","pos":30,"type":"cell"}
{"cell_type":"markdown","id":"04e353","input":"After exploring several different hyperparameter optimization tools, we found that all of them improved the `XGBregressor` model by varying amounts.  Looking just at the hyperparameter optimization of the `xgbr_model` we found that `GridSearchCV` was the most expensive with 960 model fits, but found a very good model.  `RandomSearchCV` and `BayesianOptimizaion` both used 125 model fits and `BayesianOptimization` identified the model with the lowest MSE on the test data.  However, be careful before concluding that Bayesian Optimization outperforms Random Search.  If you change the random number seeds you'll get different results and Bayesian Optimization will not always be the winner.  However, the consensus is that it works better than Random Search on average.\n\nThe last thing we ran was an AutoML experiment with TPOT which used a genetic algorithm to search over many different models and hyperparameter choices.  The model it identified was a pretty crazy nested model that didn't perform as well as the optimized `xgboost` model, but it does provide a starting point for other directions to look.  \n\nIf you're curious to explore further, there are many AutoML tools being developed.  Here are a couple of interesting ones with which you might experiment:\n\n* **AzureML from Microsoft:** Check out <a href=\"https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-auto-train-models\">this really cool tutorial</a> on using AutoML for choosing a regression model for predicting taxi fares.  The tutorial uses Python and sklearn so it wouldn't be a stretch to follow along.  Moreover, AzureML provides free credits when you sign up.  Just make sure to complete the \"Clean Up Resources\" section at the end of the tutorial so you don't leave anything running that will use up your free credits!\n\n* **RapidMiner:** We don't have personal experience with this one, but we've only heard good things about it and are eager to check it out.  <a href=\"https://rapidminer.com/educational-program/\">It is free for students.</a> RapidMiner's version of AutoML is a called Auto Model.  You can find a <a href=\"https://docs.rapidminer.com/latest/studio/auto-model/\">tutorial for predicting survival on the Titanic here.</a>\n\nIf you try any other AutoML tools, please tell us about it on Piazza.","metadata":{"hidden":true},"pos":80,"type":"cell"}
{"cell_type":"markdown","id":"0d1774","input":"We'd like to optimize the scores of our models when applied to data the model hasn't seen. However, the model doesn't see the test data during model training.  To estimate the test data model score we apply k-fold cross validation to the model training process.\n\nThis technique is taught in DS740 and you can learn more in <a href=\"https://machinelearningmastery.com/k-fold-cross-validation/\">this article</a>.  Basically the training data is divided into k subsets and the subsets are used to build models.  The scores from these models are averaged to estimate the score when applied to unseen data.  Cross validation is used by all of our hyperparameter optimization algorithms to estimate the model score and this estimated score is what we try to optimize.  \n\nWe won't really have to do cross-validation directly, but this bit of code shows how we could do it using `sklearn`.  Here we are estimating the test error of our xgboost model using 3-fold cross-validation (3 is the default number of folds for `cross_val_score`, but 5 is more commonly used for hyperparameter optimization):","pos":36,"type":"cell"}
{"cell_type":"markdown","id":"105410","input":"This number doesn't have the same clean interpretation as r-squared in a statistics setting, but it is useful for comparing models if we remember larger scores are better.  (Note:  if we were looking at mean square instead, then smaller would be better.)\n\nFinally we can use our model to make predictions:","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"1373fb","input":"Then we can pass the dictionary directly to the model constructor as shown below.  Once we created an instance of the model we also train it:","pos":63,"type":"cell"}
{"cell_type":"markdown","id":"1753b4","input":"## Setup the data","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"186bb6","input":"Now it's very easy to assess other models by first creating a model object in sklearn and then using our custom function `my_sklearn_regression`. ","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"1de6b4","input":"Fitting a model in machine learning is an optimization problem.  In a previous lesson we saw how logistic and linear regression use optimization to find the regression model coefficients to minimize difference between observed and predicted values of the response variable.  \n\nMost machine learning models also come with a bunch of parameters that need to be set which can alter the fit of the model.  For example here is the `LogisticRegression` class from scikit learn (`sklearn`):\n\n```\nclass sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n```\n\nSome of these parameters have to do with exactly what model is fit.  For instance, `penalty` changes the form of regularization added to the objective function to prevent overfitting while `C` changes the strength of the regularization (larger `C` is less regularization).  These extra parameters are usually called hyperparameters and to get the best model they often need to be tuned.  This tuning is another kind of optimization and is usually called \"hyperparameter optimization\" or \"hyperparameter tuning\".  This is a hot area and a little searching with Google will yield a ton of results.  <a href=\"https://towardsdatascience.com/understanding-hyperparameters-optimization-in-deep-learning-models-concepts-and-tools-357002a3338a\">Here is one article</a> I really like even though it is geared toward neural networks.\n\nTo keep everything straight it helps to remember that model parameters and hyperparameters are different.  Hyperparameters are set or determined before the model is fit.  Model parameters are determined during the process of fitting the model to the data.\n\nHere are four kinds of hyperparameter optimization we'll explore:\n\n* **Grid Search:**  choose a list of possible values for each hyperparameter and loop over all the combinations of hyperparameters while fitting a model for each combination.  Return the combination that gives the best model performance.  We'll use `GridSearchCV` from the `sklearn` package.  \n\n* **Random Search:** choose a list of possible values, or a probability distribution, for each hyperparameter.  Choose combinations at random and return the combination that gives the best model performance.  We'll use `RandomSearchCV` from the `sklearn` package.\n\n* **Bayesian Optimization:**  after each iteration an approximate model of the objective function, called a surrogate model, is built and used to suggest a better set of hyperparameters for the next iteration.  This is really useful for objective functions that are expensive to evaluate and for which there is uncertainty or noise in the function values as there always will be when we sample data at random to feed into a machine learning model.  We'll use `BayesianOptimization` from the `GPyOpt` package.  Other popular Python packages include `HyperOpt` and `Scikit-Optimize`.  We aren't going to study the details of Bayesian Optimization in this class, but there are a number of tutorials on the topic.  I think <a href=\"http://krasserm.github.io/2018/03/21/bayesian-optimization/\">this article</a> is an especially good place to start if you want to learn more about the details.  In particular, the section called \"Implementation with NumPy and SciPy\" walks through the process of maximize a simple $y = f(x)$.\n\n* **Genetic Algorithms:**  a population of possible hyperparameter sets is evolved using selection, crossover, and mutation with the goal of identifying hyperparameters that yield trained models with the best performance.  We'll use the TPOT package.  We'll also see that the TPOT package is really an auto machine learning package because it can pick between multiple models using the genetic algorithm.\n\nThere are **other hyperparameter optimization approaches**, but Bayesian Optimization is currently the most popular.  Work through this presentation to get an idea of how hyperparameter optimization works!\n\nIf you haven't taken DS740 - Data Mining and Machine Learning yet, you can still follow along and do the assignment, but you might wish to revisit this assignment after you've completed DS740.  We're going to walk through the four methods of hyperparameter optimization using a problem involving regression.  Your assignment will be to apply the same methods to the classification problem of predicting loan defaults using a subset of the project data from DS705 - Statistical Methods class.  ","metadata":{"hidden":true},"pos":3,"type":"cell"}
{"cell_type":"markdown","id":"31827c","input":"The diabetes dataset is included with `sklearn` as a toy dataset for learning the basics of regression analysis with a variety of different models.  Here we load the data and print the description of the data:","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"33c9d6","input":"## AutoML with TPOT","metadata":{"heading_collapsed":true},"pos":74,"type":"cell"}
{"cell_type":"markdown","id":"355192","input":"The best hyperparameter values are stored in the grid_search object as a dictionary:","pos":46,"type":"cell"}
{"cell_type":"markdown","id":"44f98b","input":"The `RandomSearchCV` and `GridSearchCV` classes both contain the model as attributes so we could just call the score() and predict() methods, but for `BayesianOptimization` we'll have to extract the coefficients and use them to build the optimized model. This is a bit fancy, but it allows us to efficiently construct the model without cutting and pasting or retyping. First we'll create a dictionary containing the optimized hyperparameter values:","pos":61,"type":"cell"}
{"cell_type":"markdown","id":"493ff6","input":"We'll train a few different models just to get an idea of how things work and to establish a baseline.  By default these models fit the data by minimizing the mean squared error which is the average squared difference between the predicted target values and the actual target values. A closely related quantity is r-squared which is the proportion of the total variation in the target values that is captured by the predicted values.\n\n**Our goal, for each regression model, is to maximize r-squared as measured on the test data.**","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"497ea9","input":"Random Forest regression uses an ensemble of decision trees and combines their final output to build a predictive model. If you haven't taken DS740 yet, that's OK.  All we really need to know is that a Random Forest regressor is a predictive model with a bunch of hyperparameters that can be changed and often are very influential in the model performance.  These models are computationally expensive to train because a typical ensemble uses 10 to 100 or more decision trees.","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"4d7905","input":"You can interpret the rmse to roughly mean that the average error in our predictions is about 52.  Below we condense the code into one cell to make it easier to follow.","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"4eecee","input":"XGBoost is a decision-tree-based ensemble algorithm that uses a gradient boosting framework to produce some pretty fantastic results.  We won't go into the details, but <a href=\"https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d\">this article</a> has a pretty nice description and talks about a variety of decision-tree based algorithms.  XGBoost is typically quite a bit faster to train than a random forest algorithm, but can still be computationally expensive.","pos":32,"type":"cell"}
{"cell_type":"markdown","id":"522787","input":"The idea behind grid search is to pick a list of potential values for each hyperparameter and then search all the possible combinations doing a k-fold cross validation for each combination.  That means we have to do k * number of combinations model fits.  If we were to run this code\n\n```\nparams = {\n    \"learning_rate\": [0.001, 0.01, 0.1, 0.5, 1.],\n    \"max_depth\": np.arange(1,11),\n    \"n_estimators\": [10,50,100],\n    \"subsample\": np.arange(0.05,1.01,0.05),\n    \"min_child_weight\": np.arange(1,21),\n    \"reg_lambda\": np.arange(0,.5,5.5),\n    \"reg_alpha\": np.arange(0,.5,5.5)\n    \n}\n\ngrid_search = GridSearchCV(xgb_model, param_grid=params, cv=5, verbose=1, return_train_score=True)\n\ngrid_search.fit(X_train,y_train)\n```\n\nwe have $k=5$ and $5 \\times 10 \\times 3 \\times 20 \\times 20 \\times 10 \\times 10 = 6,000,000$ combinations for a total of 30,000,000 model fits. Even if we could fit 10 models per second, it would still take about 34 days to try all the models.  Nevertheless, GridSearchCV is commonly used by trimming the number of possible values for each parameter to get something manageable.\n\nAs you can see the lists or arrays of values for the hyperparameters are stored in a dictionary.  The number of cross-validation folds is set by `cv = 5`.\n\nTo illustrate how this works we'll pick fewer values for each hyperparameter as shown in the next cell, but we are still doing $2^6 \\times 3 \\times 5 = 960$ model fits which takes about a minute on my Macbook Pro:","pos":44,"type":"cell"}
{"cell_type":"markdown","id":"52abf4","input":"# Hyperparameter Optimization applied to XGBoost Regression","pos":39,"type":"cell"}
{"cell_type":"markdown","id":"57b5e4","input":"Now we can finally show the how the Bayesian optimization tuned model performs on the test data:","pos":65,"type":"cell"}
{"cell_type":"markdown","id":"5ddbe2","input":"The best hyperparameters found:","pos":54,"type":"cell"}
{"cell_type":"markdown","id":"5f1b21","input":"Depending on the random seeds you've used you might see that Bayesian Optimization does better or worse than Random Search.  (Try a few different seeds and compare.  Warning - we've not been able to figure out why Bayesian Optimization gives different results with the same seed if you restart your kernel.)  However, for both the random search and for the Bayesian optimization we've done only 125 model fits as opposed to the 960 we did for grid search.\n\nBoth random search and Bayesian optimization will give better results if they're allowed to fun for more iterations.  Bayesian optimization doesn't always beat random search, but common wisdom suggests that it usually works better - see this <a href=\"https://stats.stackexchange.com/questions/302891/hyper-parameters-tuning-random-search-vs-bayesian-optimization\">Stack Exchange post</a> for some nice discussion and references.  Moreover companies are getting into automatic machine learning in a big way and some of the giants, <a href=\"https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization\">like Google, are betting on Bayesian Optimization</a>.  ","pos":67,"type":"cell"}
{"cell_type":"markdown","id":"6672cb","input":"One nice feature of TPOT is that it can write the optimized model to a file which you can use elsewhere.\n\nWe can display the results on the test data in the same way as with our other models.","metadata":{"hidden":true},"pos":71,"type":"cell"}
{"cell_type":"markdown","id":"668463","input":"Optimize a random forest regression model and a XGboost classification model by completing the work in the `Project_2_Report.ipynb` notebook.\n\n# Supplemental Material\nWhile we've provided a balanced dataset for you, an example of a classification problem with imbalanced data is included in the supplemental notebook, Project_02_Supplement_Imbalanced_Classification, for those that might like to go further and/or use their own dataset.","metadata":{"hidden":true},"pos":82,"type":"cell"}
{"cell_type":"markdown","id":"681c9c","input":"# The Regression Problem","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"6832d2","input":"If we can only afford to fit the model a limited number of times, then one approach is to search randomly instead of exhaustively.  To use `RandomizedSearchCV` we can either specify a probability distribution for each hyperparameter or we can specify a list of values for the hyperparameter in which case a value is chosen from the list assuming all values in the list are equally probable. \n\nFor optimizing our XGBoost model Wwe'll leave the learning rate as a list since we want more small values to choose from than large values.  The other hyperparameters can be specified with distributions.  Note that the uniform distribution specified below is not intuitive.  `uniform(loc,scale)` is uniform on the interval `[loc, loc+scale]`.  For the search below we're going to check just 25 randomly selected sets of hyperparameters as we might for a really expensive model.   `random_state = 3` is a random number seed for reproducibility.  Change it and you'll get different results.  ","pos":52,"type":"cell"}
{"cell_type":"markdown","id":"6b46f9","input":"If you were to run TPOT again with a different random seed or with different settings you'd very likely find a different model.  It often produces nested models where models are applied in sequence. \n\nUnderstanding the details of nested models and such isn't important here and we don't recommend blindly using AutoML of any sort, but TPOT can provide good starting points and suggestions for models to investigate further.  We're eager to try other AutoML tools to see how they work.","metadata":{"hidden":true},"pos":78,"type":"cell"}
{"cell_type":"markdown","id":"6fd721","input":"# Introduction","metadata":{"heading_collapsed":true},"pos":2,"type":"cell"}
{"cell_type":"markdown","id":"78f25f","input":"On average Bayesian optimization does better than random search.  It especially excels when there are lots of hyperparameters, but it won't beat random search every time.  The power of Bayesian optimization is that it can often achieve good results with a relatively short number of training iterations.  \n\nThe setup is a bit more complicated.  We start by specifying the bounds as a list of dictionaries.  For each hyperparameter we give a name, the type, and the range.  If you want to use strings or non-equally space values then you have to work a bit harder, but we shouldn't need that for this project.\n\nThe function `cv_score()` receives a nested list with hyperparameter values that we use to setup the model.  We have to manually map the hyperparameter values to their places in the model declaration as shown below.  `cv_score` returns the 5-fold cross-validated estimate of the model score.  \n\nIn essence `cv_score()` is just like `rastrigin(x)` or any other objective function.  We now pass the name of the objective function to our optimization routine.  To make this comparable to the random search we want to do 25 iterations resulting in 125 model fits, but we've declared the number of iterations to be 20 since `BayesianOptimization` starts by training the model at 5 random sets of hyperparameter values.  ","pos":59,"type":"cell"}
{"cell_type":"markdown","id":"79623a","input":"### Linear Regression and sklearn basics","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"8367bd","input":"We really don't need to do much with genetic algorithms to use TPOT, though we can change the usual parameters like population size, mutation probability, and crossover probability.  The software authors recommend leaving the probabilities at their default values - <a href=\"https://epistasislab.github.io/tpot/\"> the documentation is here.</a> \n\nTPOT can actually do much more than optimize hyperparameters for a single model, but it can do that too.  To focus on a single model we set up a nested dictionary like that shown in the code below.    Then we call TPOT and it returns an optimized model in an object that behaves just like objects returned by GridSearchCV and RandomSearchCV.  Additional models could be added as `'model_name':{'param':values,'param':value,...}`.\n\nWe've found that we generally need more model fits to get good results with TPOT than we did with Bayesian Optimization, but it still works really well.  Note that TPOT is maximizing the k-fold cross-validated negative mean square error instead of r-squared, but it gets us to the same place.  Here we iterate for 5 generations with 20 different individual sets of hyperameters in each generation.  For each individual we do 5 model fits (k = 5) and there is an extra round of cross validated fits for the initial population, thus altogether we perform $6 \\times 20 \\times 5 = 600$ model fits.","metadata":{"hidden":true},"pos":69,"type":"cell"}
{"cell_type":"markdown","id":"86e41d","input":"From the plot we can see that the model is far from perfect, but it is getting the overall trend right.  One thing to note is that it's doing a pretty poor job at predicting large target values.  \n\nFor a real-world problem we'd want to assess the accuracy of the model predictions on the test data.  For regression problems this is often done with the mean square error or root mean square error (rmse):","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"8e77c6","input":"## Using RandomizedSearchCV","pos":51,"type":"cell"}
{"cell_type":"markdown","id":"9287c5","input":"Several of the values are different than their default values.  To see if this optimized model is better than the default XGBoost model let's apply it to the test data:\n","pos":48,"type":"cell"}
{"cell_type":"markdown","id":"a5d6e4","input":"**Our objective is to predict the target variable from the other 10 features.**","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"a6fecd","input":"The random search found results that were not quite as good as those found with the grid search, but with far fewer model fits.","pos":57,"type":"cell"}
{"cell_type":"markdown","id":"a8f434","input":"### XGBoost","pos":31,"type":"cell"}
{"cell_type":"markdown","id":"ad8e24","input":"## Using BayesianOptimization from GPyOpt","pos":58,"type":"cell"}
{"cell_type":"markdown","id":"b46e3b","input":"Fortunately the default values shown above work pretty well in many problems.  Some of the hyperparameters don't directly change the model like `nthread` and `verbosity`.  Of the rest we'll pick a subset to optimize.  Some commonly optimized parameters are `n_estimators`, `max_depth`, `learning_rate`, `subsample`, and `min_child_weight` (these are the same ones that are optimized in the `TPOT` package). Two other hyperparameters linked to regularization terms are `reg_lambda` and `reg_alpha` which can be useful to prevent overfitting.\n\nThe table below lists some typical values and default values:\n\nHyperparameter | Default Value | Typical Range\n---- | ---- | ----\nn_estimators | 100 | 10 to 100\nmax_depth | 3 | 1 to 10\nmin_child_weight | 1 | 1 to 20\nlearning_rate | 0.1 | 0.001 to 1\nsub_sample | 1 | 0.05 to 1\nreg_lambda | 1 | 0 to 5\nreg_alpha  | 0 | 0 to 5\n\nOf course, we could throw more hyperparameters into the mix, but we'll keep the numbers down to so we can afford to experiment.  ","pos":42,"type":"cell"}
{"cell_type":"markdown","id":"b95c3b","input":"# Estimating the model score without test data","pos":35,"type":"cell"}
{"cell_type":"markdown","id":"bdf1a3","input":"## Using GridSearchCV","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"c50d4a","input":"We've actually used TPOT in a rather narrow way by forcing it to optimize the hyperparameters for one choice of a machine learning model.  However, TPOT is really designed as an auto machine learning tool (AutoML) that tries to figure out optimize the whole machine learning pipeline:  data preprocessing, feature selection, model selection, and hyperparamter tuning.  For real problems this process could take days (see the <a href=\"https://epistasislab.github.io/tpot/using/#what-to-expect-from-automl-software\">TPOT discussion of AutoML</a>.  For this toy problem it doesn't take too long so let's see what it does.  \n\nBy specifying `None` for the config_dict parameter <a href=\"https://epistasislab.github.io/tpot/using/#built-in-tpot-configurations\">TPOT defaults</a> to optimizing the whole machine learning pipeline.  We'll turn it loose with a population size of 40 for 10 generations which will require 2400 model fits (roughly 5 minutes on my Macbook):","metadata":{"hidden":true},"pos":75,"type":"cell"}
{"cell_type":"markdown","id":"c7ee0e","input":"### Random Forest Regression","pos":27,"type":"cell"}
{"cell_type":"markdown","id":"c83b7d","input":"We're not going to do much with this data.  In a full machine learning analysis you would also want explore the data, do feature selection and possibly feature engineering including variable rescaling.  We're going to focus on only hyperparameter optimization.\n\nWe've already loaded the data, but we'll grab our features, `X`, and our target variable `y`.  We could just fit a model to all of the data, but we don't want a model that's just memorized the data and can't generalize to new data (overfitting).  So we usually divide the data into test and training sets.  The training set is used to fit a model, while the test set is used to validate the result.  Typically around 20% of the data is randomly selected for the test set.  We set the random number seed in `train_test_split` for reproducibility.","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"ccdfa9","input":"We'll focus on the XGboost model for regression because it's a pretty amazing model.  If you haven't heard about it, then try to Google a bit or <a href=\"https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/\">check out this article</a> to learn more.  The XGboost model has many hyperparameters:","pos":40,"type":"cell"}
{"cell_type":"markdown","id":"cead0a","input":"**Here is the important bit about cross-validation** for estimating model performance:  *the k-fold cross-validated model score is the quantity we optimize in hyperparameter optimization*.\n\nFor regression problems we are usually minimizing the k-fold cross-validated mean square error.  For classification problems we maximize the k-fold cross-validated accuracy where accuracy is the total number of correct classifications divided by the total number of classifications.  The number of folds used is commonly $k=5$ or $k=10$, but we'll mostly use $k=3$ just to speed things up for learning purposes.","pos":38,"type":"cell"}
{"cell_type":"markdown","id":"ceae46","input":"<font size=18>Project 2: Hyperparameter Optimization for Machine Learning</font>","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"cf2ae9","input":"This is the same multiple linear regression model you've learned about in statistics.  We saw a bit about fitting a simple linear regression model in Python as part of Lesson 4.  Fitting a model in `sklearn` is pretty straightforward.","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"db39d8","input":"Notice that the r-squared score is little higher than that of the random forest model, but not as high as for the linear regression model.  This model is slightly better than than the random forest model, but not as good as the linear regression model.\n\nOur best model thus far is the linear regression model.  Perhaps using the default hyperparameter values for training the other models wasn't the best choice.  In what follows, we'll try to optimize improve xgboost model by tuning its parameters.","pos":34,"type":"cell"}
{"cell_type":"markdown","id":"df64c2","input":"## Using a Genetic Algorithm from TPOT","metadata":{"heading_collapsed":true},"pos":68,"type":"cell"}
{"cell_type":"markdown","id":"e0570d","input":"This value of r-squared is the same as the value you met in statistics.  We interpret this to mean that our linear regression model captures 50% of the variation in the training data.  However, in a machine learning context we want to know how this model works on new data.  \n\nIf we apply the score() method to the test data it's no longer the value of r-squared that we learned about in a statistics class.  *This is because we are evaluating r-squared score on data that was not used to build the model.* For example, we can get a negative r-squared number which indicates that the model is performing worse, on the test data, than simply predicting that all of the target values are the same as the average target value.  \n\nIn short, when we compute the r-squared score() for the test data, values near one are great.  Values near zero just mean the model isn't really helping and negative values mean that are model is worse than no model at all.  This is the metric we will use to select our regression models so **bigger is better**.  Here is how we compute it:","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"ec0c2e","input":"In a basic machine learning pipeline we could look at the model score.  This is basically the objective function value being optimized by sci-kit (or a quantity derived from that).  For regression models we usually look at either\n* mean squared error: the average squared difference between the true and predicted target values \nor\n* r-squared: the proportion of the total variation in the target values that is accounted for by the model\n\n### Mathematical representation of $R^2$ and MSE:\n\nDefinitions:\n* $y_{i}$ = actual i*th* observation\n* $f_{i}$ = predicted i*th* observation\n* $\\bar{y}$ = mean of actual observations\n* $n$ = degrees of freedom (or num observations)\n\n**Total sum of squares** (proportional to the variance of the data):\n\n$ SS_{total} = \\displaystyle \\sum_{i} (y_{i} - \\bar{y})^2$\n\n**Residuals sum of squares** , also called sum of squares residuals:\n\n$ SS_{res} = \\displaystyle \\sum_{i} (y_{i} - f_{i})^2$\n\n**Mean squared error**:\n\nMSE = $SS_{res}\\over n $\n\n#### The most general definition of the coefficient of determination is:\n\n$ R^2 = 1 - {SS_{res}\\over SS_{tot}}$\n\nFor regression, if we compute the score of the model applied to the training data:","pos":16,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"ed7860","input":"In this example TPOT did not do as well as `BayesianOptimization`, but TPOT can do much more as we'll see in the next section.","metadata":{"hidden":true},"pos":73,"type":"cell"}
{"cell_type":"markdown","id":"f993f6","input":"## Establishing a baseline","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"fb3922","input":"# Assignment","metadata":{"heading_collapsed":true},"pos":81,"type":"cell"}
{"cell_type":"markdown","id":"fcbe38","input":"# Summary","metadata":{"heading_collapsed":true},"pos":79,"type":"cell"}
{"id":0,"time":1597679175984,"type":"user"}
{"last_load":1597679159862,"type":"file"}