{"backend_state":"running","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-aa7d8a4b-2059-4ea2-bb59-f9293948de84.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"234.363px"},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1610068379311,"exec_count":1,"id":"476ca7","input":"# execute to import notebook styling for tables and width etc.\nfrom IPython.core.display import HTML\nimport urllib.request\nresponse = urllib.request.urlopen('https://raw.githubusercontent.com/DataScienceUWL/DS775v2/master/ds755.css')\nHTML(response.read().decode(\"utf-8\"));","kernel":"python3","metadata":{"code_folding":[0]},"no_halt":true,"pos":0,"start":1610068379103,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068379833,"exec_count":2,"id":"b16af9","input":"# load the data\nimport pandas as pd\nimport numpy as np\nbx = pd.read_csv('./data/BX-Book-Ratings-3000.csv')\nbx.head()","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User-ID</th>\n      <th>ISBN</th>\n      <th>Book-Rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6251</td>\n      <td>345370775</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6251</td>\n      <td>044021145X</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6251</td>\n      <td>312983271</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6251</td>\n      <td>080410526X</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6251</td>\n      <td>743418174</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   User-ID        ISBN  Book-Rating\n0     6251   345370775            1\n1     6251  044021145X            1\n2     6251   312983271            1\n3     6251  080410526X            1\n4     6251   743418174            1"},"exec_count":2}},"pos":3,"start":1610068379322,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068379851,"exec_count":3,"id":"62194f","input":"print(\"Mean book rating:     \", '%.2f' % bx['Book-Rating'].mean())","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"output":{"0":{"name":"stdout","text":"Mean book rating:      2.63\n"}},"pos":4,"start":1610068379839,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068382723,"exec_count":4,"id":"c23dda","input":"#Import the train_test_split function\nfrom sklearn.model_selection import train_test_split\n\n#Assign X as the original ratings dataframe and y as the user_id column of ratings.\nX = bx.copy()\ny = bx['User-ID']\n\n#Split into training and test datasets, stratified along user_id\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, stratify=y, random_state=42)","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"pos":5,"start":1610068379863,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068382736,"exec_count":5,"id":"fccc5c","input":"#Import the mean_squared_error function\nfrom sklearn.metrics import mean_squared_error\n\n#verify the median of the data\nprint(f\"The median of this rating range is {np.median(np.arange(np.min(bx['Book-Rating']), (np.max(bx['Book-Rating']) + 1)))}\")\n\n#Define the baseline model to always return 5.\ndef baseline(user_id, book_rating, *args):\n    return 6.0","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"output":{"0":{"name":"stdout","text":"The median of this rating range is 6.0\n"}},"pos":7,"start":1610068382732,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068382813,"exec_count":6,"id":"58151a","input":"#Function to compute the RMSE score obtained on the testing set by a model\ndef score(cf_model, X_test, *args):\n    \n    #Construct a list of user-book tuples from the testing dataset\n    id_pairs = zip(X_test['User-ID'], X_test['ISBN'])\n    \n    #Predict the rating for every user-book tuple\n    y_pred = np.array([cf_model(user, book, *args) for (user, book) in id_pairs])\n    \n    #Extract the actual ratings given by the users in the test data\n    y_true = np.array(X_test['Book-Rating'])\n    \n    #Return the final RMSE score\n    return mean_squared_error(y_true, y_pred, squared=False)","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"pos":8,"start":1610068382744,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068382884,"exec_count":7,"id":"300230","input":"score(baseline, X_test)","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"output":{"0":{"data":{"text/plain":"4.703780985075257"},"exec_count":7}},"pos":9,"start":1610068382819,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068383010,"exec_count":8,"id":"081409","input":"#Build the ratings matrix using pivot_table function\n#r_matrix = X_train.pivot_table(values='Book-Rating', index='User-ID', columns='ISBN')\nr_matrix = X_train.pivot(values='Book-Rating', index='User-ID', columns='ISBN')\n\nr_matrix.head()","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>ISBN</th>\n      <th>006101351X</th>\n      <th>014025448X</th>\n      <th>014028009X</th>\n      <th>034540288X</th>\n      <th>038079487X</th>\n      <th>043935806X</th>\n      <th>044021145X</th>\n      <th>044022165X</th>\n      <th>044023722X</th>\n      <th>044651652X</th>\n      <th>...</th>\n      <th>743418174</th>\n      <th>767902521</th>\n      <th>767905180</th>\n      <th>786868716</th>\n      <th>786881852</th>\n      <th>804106304</th>\n      <th>804114986</th>\n      <th>805063897</th>\n      <th>842329129</th>\n      <th>971880107</th>\n    </tr>\n    <tr>\n      <th>User-ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6251</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6575</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7346</th>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11601</th>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11676</th>\n      <td>9.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 200 columns</p>\n</div>","text/plain":"ISBN     006101351X  014025448X  014028009X  034540288X  038079487X  \\\nUser-ID                                                               \n6251            NaN         NaN         NaN         1.0         NaN   \n6575            NaN         NaN         NaN         NaN         NaN   \n7346            1.0         NaN         NaN         NaN         NaN   \n11601           NaN         1.0         NaN         NaN         NaN   \n11676           9.0         NaN         NaN         NaN         NaN   \n\nISBN     043935806X  044021145X  044022165X  044023722X  044651652X  ...  \\\nUser-ID                                                              ...   \n6251            NaN         1.0         NaN         NaN         NaN  ...   \n6575            NaN         NaN         NaN         NaN         NaN  ...   \n7346            NaN         NaN         NaN         NaN         NaN  ...   \n11601           NaN         NaN         NaN         NaN         NaN  ...   \n11676           NaN         NaN         NaN         NaN         NaN  ...   \n\nISBN     743418174  767902521  767905180  786868716  786881852  804106304  \\\nUser-ID                                                                     \n6251           1.0        NaN        NaN        1.0        NaN        NaN   \n6575           NaN        NaN        NaN        NaN        NaN        NaN   \n7346           NaN        NaN        NaN        NaN        NaN       10.0   \n11601          NaN        NaN        NaN        NaN        NaN        NaN   \n11676          NaN        NaN        NaN        NaN        NaN        NaN   \n\nISBN     804114986  805063897  842329129  971880107  \nUser-ID                                              \n6251           NaN        NaN        NaN        NaN  \n6575           NaN        NaN        NaN        NaN  \n7346           1.0        NaN        NaN        NaN  \n11601          NaN        NaN        NaN        NaN  \n11676          NaN        NaN        NaN        NaN  \n\n[5 rows x 200 columns]"},"exec_count":8}},"pos":11,"start":1610068382894,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068383054,"exec_count":9,"id":"c66a8d","input":"#Create a dummy ratings matrix with all null values imputed to 0\nr_matrix_dummy = r_matrix.copy().fillna(0)","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"pos":12,"start":1610068383015,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068383168,"exec_count":10,"id":"9fb52b","input":"r_matrix_dummy.head()","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>ISBN</th>\n      <th>006101351X</th>\n      <th>014025448X</th>\n      <th>014028009X</th>\n      <th>034540288X</th>\n      <th>038079487X</th>\n      <th>043935806X</th>\n      <th>044021145X</th>\n      <th>044022165X</th>\n      <th>044023722X</th>\n      <th>044651652X</th>\n      <th>...</th>\n      <th>743418174</th>\n      <th>767902521</th>\n      <th>767905180</th>\n      <th>786868716</th>\n      <th>786881852</th>\n      <th>804106304</th>\n      <th>804114986</th>\n      <th>805063897</th>\n      <th>842329129</th>\n      <th>971880107</th>\n    </tr>\n    <tr>\n      <th>User-ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6251</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6575</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7346</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>11601</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>11676</th>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 200 columns</p>\n</div>","text/plain":"ISBN     006101351X  014025448X  014028009X  034540288X  038079487X  \\\nUser-ID                                                               \n6251            0.0         0.0         0.0         1.0         0.0   \n6575            0.0         0.0         0.0         0.0         0.0   \n7346            1.0         0.0         0.0         0.0         0.0   \n11601           0.0         1.0         0.0         0.0         0.0   \n11676           9.0         0.0         0.0         0.0         0.0   \n\nISBN     043935806X  044021145X  044022165X  044023722X  044651652X  ...  \\\nUser-ID                                                              ...   \n6251            0.0         1.0         0.0         0.0         0.0  ...   \n6575            0.0         0.0         0.0         0.0         0.0  ...   \n7346            0.0         0.0         0.0         0.0         0.0  ...   \n11601           0.0         0.0         0.0         0.0         0.0  ...   \n11676           0.0         0.0         0.0         0.0         0.0  ...   \n\nISBN     743418174  767902521  767905180  786868716  786881852  804106304  \\\nUser-ID                                                                     \n6251           1.0        0.0        0.0        1.0        0.0        0.0   \n6575           0.0        0.0        0.0        0.0        0.0        0.0   \n7346           0.0        0.0        0.0        0.0        0.0       10.0   \n11601          0.0        0.0        0.0        0.0        0.0        0.0   \n11676          0.0        0.0        0.0        0.0        0.0        0.0   \n\nISBN     804114986  805063897  842329129  971880107  \nUser-ID                                              \n6251           0.0        0.0        0.0        0.0  \n6575           0.0        0.0        0.0        0.0  \n7346           1.0        0.0        0.0        0.0  \n11601          0.0        0.0        0.0        0.0  \n11676          0.0        0.0        0.0        0.0  \n\n[5 rows x 200 columns]"},"exec_count":10}},"pos":13,"scrolled":true,"start":1610068383058,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068383261,"exec_count":11,"id":"f0fdac","input":"# Import cosine_score \nfrom sklearn.metrics.pairwise import cosine_similarity\n\n#Compute the cosine similarity matrix using the dummy ratings matrix\ncosine_sim = cosine_similarity(r_matrix_dummy, r_matrix_dummy)","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"pos":14,"start":1610068383173,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068383460,"exec_count":12,"id":"9cf89f","input":"#Convert into pandas dataframe \ncosine_sim = pd.DataFrame(cosine_sim, index=r_matrix.index, columns=r_matrix.index)\n\ncosine_sim.head()","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>User-ID</th>\n      <th>6251</th>\n      <th>6575</th>\n      <th>7346</th>\n      <th>11601</th>\n      <th>11676</th>\n      <th>13552</th>\n      <th>14521</th>\n      <th>16795</th>\n      <th>21014</th>\n      <th>23768</th>\n      <th>...</th>\n      <th>238781</th>\n      <th>254465</th>\n      <th>258534</th>\n      <th>260897</th>\n      <th>261829</th>\n      <th>265115</th>\n      <th>265313</th>\n      <th>266226</th>\n      <th>269566</th>\n      <th>274308</th>\n    </tr>\n    <tr>\n      <th>User-ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6251</th>\n      <td>1.000000</td>\n      <td>0.019892</td>\n      <td>0.030961</td>\n      <td>0.005078</td>\n      <td>0.142988</td>\n      <td>0.048059</td>\n      <td>0.064752</td>\n      <td>0.018610</td>\n      <td>0.002779</td>\n      <td>0.035858</td>\n      <td>...</td>\n      <td>0.015235</td>\n      <td>0.023108</td>\n      <td>0.006664</td>\n      <td>0.026021</td>\n      <td>0.018473</td>\n      <td>0.029876</td>\n      <td>0.003219</td>\n      <td>0.052636</td>\n      <td>0.079578</td>\n      <td>0.045950</td>\n    </tr>\n    <tr>\n      <th>6575</th>\n      <td>0.019892</td>\n      <td>1.000000</td>\n      <td>0.001540</td>\n      <td>0.022224</td>\n      <td>0.115155</td>\n      <td>0.002390</td>\n      <td>0.000000</td>\n      <td>0.001018</td>\n      <td>0.132236</td>\n      <td>0.046586</td>\n      <td>...</td>\n      <td>0.075006</td>\n      <td>0.145943</td>\n      <td>0.014581</td>\n      <td>0.001582</td>\n      <td>0.000000</td>\n      <td>0.012711</td>\n      <td>0.019368</td>\n      <td>0.014397</td>\n      <td>0.051694</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>7346</th>\n      <td>0.030961</td>\n      <td>0.001540</td>\n      <td>1.000000</td>\n      <td>0.003931</td>\n      <td>0.193686</td>\n      <td>0.016909</td>\n      <td>0.019491</td>\n      <td>0.024488</td>\n      <td>0.002151</td>\n      <td>0.104082</td>\n      <td>...</td>\n      <td>0.043239</td>\n      <td>0.030895</td>\n      <td>0.000000</td>\n      <td>0.044759</td>\n      <td>0.017873</td>\n      <td>0.256943</td>\n      <td>0.007474</td>\n      <td>0.112041</td>\n      <td>0.281033</td>\n      <td>0.032603</td>\n    </tr>\n    <tr>\n      <th>11601</th>\n      <td>0.005078</td>\n      <td>0.022224</td>\n      <td>0.003931</td>\n      <td>1.000000</td>\n      <td>0.002773</td>\n      <td>0.030508</td>\n      <td>0.005024</td>\n      <td>0.031187</td>\n      <td>0.003880</td>\n      <td>0.006260</td>\n      <td>...</td>\n      <td>0.007092</td>\n      <td>0.002934</td>\n      <td>0.006204</td>\n      <td>0.407819</td>\n      <td>0.029023</td>\n      <td>0.046359</td>\n      <td>0.000000</td>\n      <td>0.055132</td>\n      <td>0.020838</td>\n      <td>0.085563</td>\n    </tr>\n    <tr>\n      <th>11676</th>\n      <td>0.142988</td>\n      <td>0.115155</td>\n      <td>0.193686</td>\n      <td>0.002773</td>\n      <td>1.000000</td>\n      <td>0.009544</td>\n      <td>0.212180</td>\n      <td>0.081307</td>\n      <td>0.022762</td>\n      <td>0.088125</td>\n      <td>...</td>\n      <td>0.033282</td>\n      <td>0.022946</td>\n      <td>0.000000</td>\n      <td>0.018948</td>\n      <td>0.001261</td>\n      <td>0.010877</td>\n      <td>0.275982</td>\n      <td>0.079053</td>\n      <td>0.027163</td>\n      <td>0.043917</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 100 columns</p>\n</div>","text/plain":"User-ID    6251      6575      7346      11601     11676     13552     14521   \\\nUser-ID                                                                         \n6251     1.000000  0.019892  0.030961  0.005078  0.142988  0.048059  0.064752   \n6575     0.019892  1.000000  0.001540  0.022224  0.115155  0.002390  0.000000   \n7346     0.030961  0.001540  1.000000  0.003931  0.193686  0.016909  0.019491   \n11601    0.005078  0.022224  0.003931  1.000000  0.002773  0.030508  0.005024   \n11676    0.142988  0.115155  0.193686  0.002773  1.000000  0.009544  0.212180   \n\nUser-ID    16795     21014     23768   ...    238781    254465    258534  \\\nUser-ID                                ...                                 \n6251     0.018610  0.002779  0.035858  ...  0.015235  0.023108  0.006664   \n6575     0.001018  0.132236  0.046586  ...  0.075006  0.145943  0.014581   \n7346     0.024488  0.002151  0.104082  ...  0.043239  0.030895  0.000000   \n11601    0.031187  0.003880  0.006260  ...  0.007092  0.002934  0.006204   \n11676    0.081307  0.022762  0.088125  ...  0.033282  0.022946  0.000000   \n\nUser-ID    260897    261829    265115    265313    266226    269566    274308  \nUser-ID                                                                        \n6251     0.026021  0.018473  0.029876  0.003219  0.052636  0.079578  0.045950  \n6575     0.001582  0.000000  0.012711  0.019368  0.014397  0.051694  0.000000  \n7346     0.044759  0.017873  0.256943  0.007474  0.112041  0.281033  0.032603  \n11601    0.407819  0.029023  0.046359  0.000000  0.055132  0.020838  0.085563  \n11676    0.018948  0.001261  0.010877  0.275982  0.079053  0.027163  0.043917  \n\n[5 rows x 100 columns]"},"exec_count":12}},"pos":15,"start":1610068383271,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068383596,"exec_count":13,"id":"9e3972","input":"#User Based Collaborative Filter using Weighted Mean Ratings\ndef cf_user_wmean(user_id, book_id, ratings_matrix, c_sim_matrix):\n    \n    #Check if book_id exists in r_matrix and if there is overlap with other \n    # users, \n#    sim_scores = cosine_sim[user_id]\n#    sss = sum(sim_scores)\n    if book_id in ratings_matrix:\n        \n        #Get the similarity scores for the user in question with every other user\n        sim_scores = c_sim_matrix[user_id]\n        \n        #Get the user ratings for the movie in question\n        m_ratings = ratings_matrix[book_id]\n        \n        #Extract the indices containing NaN in the m_ratings series\n        idx = m_ratings[m_ratings.isnull()].index\n                \n        #Drop the NaN values from the m_ratings Series\n        m_ratings = m_ratings.dropna()\n        \n        #Drop the corresponding cosine scores from the sim_scores series\n        sim_scores = sim_scores.drop(idx)\n        \n        #Compute the final weighted mean\n        if sim_scores.sum()>0:\n            wmean_rating = np.dot(sim_scores, m_ratings)/ sim_scores.sum()\n        else:  # user had zero cosine similarity with other users\n            wmean_rating = 6.0\n    \n    else:\n        #Default to a rating of 6.0 in the absence of any information\n        wmean_rating = 6.0\n    \n    return wmean_rating","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"pos":16,"start":1610068383477,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068384647,"exec_count":14,"id":"2514d3","input":"score(cf_user_wmean, X_test, r_matrix, cosine_sim)","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"output":{"0":{"data":{"text/plain":"3.607093266358255"},"exec_count":14}},"pos":17,"start":1610068383602,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068384682,"exec_count":15,"id":"6f0ec6","input":"#Build the ratings matrix using pivot_table function\n#r_matrix = X_train.pivot_table(values='Book-Rating', index='ISBN', columns='User-ID')\nr_matrix_item = X_train.pivot(values='Book-Rating', index='ISBN', columns='User-ID')\n\nr_matrix_item.head()","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>User-ID</th>\n      <th>6251</th>\n      <th>6575</th>\n      <th>7346</th>\n      <th>11601</th>\n      <th>11676</th>\n      <th>13552</th>\n      <th>14521</th>\n      <th>16795</th>\n      <th>21014</th>\n      <th>23768</th>\n      <th>...</th>\n      <th>238781</th>\n      <th>254465</th>\n      <th>258534</th>\n      <th>260897</th>\n      <th>261829</th>\n      <th>265115</th>\n      <th>265313</th>\n      <th>266226</th>\n      <th>269566</th>\n      <th>274308</th>\n    </tr>\n    <tr>\n      <th>ISBN</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>006101351X</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>9.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>014025448X</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>014028009X</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>034540288X</th>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>038079487X</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 100 columns</p>\n</div>","text/plain":"User-ID     6251    6575    7346    11601   11676   13552   14521   16795   \\\nISBN                                                                         \n006101351X     NaN     NaN     1.0     NaN     9.0     NaN     6.0     NaN   \n014025448X     NaN     NaN     NaN     1.0     NaN     NaN     NaN     NaN   \n014028009X     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n034540288X     1.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n038079487X     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n\nUser-ID     21014   23768   ...  238781  254465  258534  260897  261829  \\\nISBN                        ...                                           \n006101351X     NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n014025448X     NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n014028009X     NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n034540288X     NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n038079487X     NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n\nUser-ID     265115  265313  266226  269566  274308  \nISBN                                                \n006101351X     NaN     NaN     1.0     NaN     1.0  \n014025448X     8.0     NaN     NaN     NaN     NaN  \n014028009X     NaN     NaN     NaN     NaN     NaN  \n034540288X     NaN     NaN     NaN     NaN     1.0  \n038079487X     1.0     NaN     NaN     NaN     NaN  \n\n[5 rows x 100 columns]"},"exec_count":15}},"pos":20,"start":1610068384653,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068384776,"exec_count":16,"id":"e7e48b","input":"#Create a dummy ratings matrix with all null values imputed to 0\nr_matrix_item_dummy = r_matrix_item.copy().fillna(0)","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"pos":21,"start":1610068384733,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068384940,"exec_count":17,"id":"137835","input":"# Import cosine_score \nfrom sklearn.metrics.pairwise import cosine_similarity\n\n#Compute the cosine similarity matrix using the dummy ratings matrix\ncosine_sim_item = cosine_similarity(r_matrix_item_dummy, r_matrix_item_dummy)","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"pos":22,"start":1610068384781,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068385352,"exec_count":18,"id":"27c21d","input":"#Convert into pandas dataframe \ncosine_sim_item = pd.DataFrame(cosine_sim_item, index=r_matrix_item.index, columns=r_matrix_item.index)\n\ncosine_sim_item.head(10)","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>ISBN</th>\n      <th>006101351X</th>\n      <th>014025448X</th>\n      <th>014028009X</th>\n      <th>034540288X</th>\n      <th>038079487X</th>\n      <th>043935806X</th>\n      <th>044021145X</th>\n      <th>044022165X</th>\n      <th>044023722X</th>\n      <th>044651652X</th>\n      <th>...</th>\n      <th>743418174</th>\n      <th>767902521</th>\n      <th>767905180</th>\n      <th>786868716</th>\n      <th>786881852</th>\n      <th>804106304</th>\n      <th>804114986</th>\n      <th>805063897</th>\n      <th>842329129</th>\n      <th>971880107</th>\n    </tr>\n    <tr>\n      <th>ISBN</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>006101351X</th>\n      <td>1.000000</td>\n      <td>0.005847</td>\n      <td>0.044958</td>\n      <td>0.035806</td>\n      <td>0.000000</td>\n      <td>0.051717</td>\n      <td>0.036379</td>\n      <td>0.032858</td>\n      <td>0.044481</td>\n      <td>0.064775</td>\n      <td>...</td>\n      <td>0.032187</td>\n      <td>0.047741</td>\n      <td>0.009323</td>\n      <td>0.007262</td>\n      <td>0.006989</td>\n      <td>0.058881</td>\n      <td>0.043183</td>\n      <td>0.029235</td>\n      <td>0.006989</td>\n      <td>0.026854</td>\n    </tr>\n    <tr>\n      <th>014025448X</th>\n      <td>0.005847</td>\n      <td>1.000000</td>\n      <td>0.009320</td>\n      <td>0.000000</td>\n      <td>0.044639</td>\n      <td>0.000000</td>\n      <td>0.031109</td>\n      <td>0.000000</td>\n      <td>0.007245</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.033029</td>\n      <td>0.049897</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.071714</td>\n      <td>0.010206</td>\n    </tr>\n    <tr>\n      <th>014028009X</th>\n      <td>0.044958</td>\n      <td>0.009320</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.032617</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.034416</td>\n      <td>...</td>\n      <td>0.030783</td>\n      <td>0.006341</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.391058</td>\n      <td>0.189290</td>\n      <td>0.279600</td>\n      <td>0.000000</td>\n      <td>0.007134</td>\n    </tr>\n    <tr>\n      <th>034540288X</th>\n      <td>0.035806</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.032827</td>\n      <td>0.063500</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.150756</td>\n      <td>...</td>\n      <td>0.044947</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.025351</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.048795</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>038079487X</th>\n      <td>0.000000</td>\n      <td>0.044639</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.055744</td>\n      <td>0.177880</td>\n      <td>0.146524</td>\n      <td>...</td>\n      <td>0.051876</td>\n      <td>0.033748</td>\n      <td>0.063267</td>\n      <td>0.024639</td>\n      <td>0.000000</td>\n      <td>0.049947</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.539464</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>043935806X</th>\n      <td>0.051717</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.032827</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.041690</td>\n      <td>0.005021</td>\n      <td>0.034955</td>\n      <td>0.197952</td>\n      <td>...</td>\n      <td>0.014754</td>\n      <td>0.328266</td>\n      <td>0.000000</td>\n      <td>0.003329</td>\n      <td>0.032035</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.006407</td>\n      <td>0.131306</td>\n    </tr>\n    <tr>\n      <th>044021145X</th>\n      <td>0.036379</td>\n      <td>0.031109</td>\n      <td>0.032617</td>\n      <td>0.063500</td>\n      <td>0.000000</td>\n      <td>0.041690</td>\n      <td>1.000000</td>\n      <td>0.019424</td>\n      <td>0.005635</td>\n      <td>0.191460</td>\n      <td>...</td>\n      <td>0.031395</td>\n      <td>0.038806</td>\n      <td>0.066136</td>\n      <td>0.003220</td>\n      <td>0.006197</td>\n      <td>0.000000</td>\n      <td>0.191460</td>\n      <td>0.000000</td>\n      <td>0.037182</td>\n      <td>0.007938</td>\n    </tr>\n    <tr>\n      <th>044022165X</th>\n      <td>0.032858</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.055744</td>\n      <td>0.005021</td>\n      <td>0.019424</td>\n      <td>1.000000</td>\n      <td>0.020357</td>\n      <td>0.023057</td>\n      <td>...</td>\n      <td>0.175295</td>\n      <td>0.000000</td>\n      <td>0.039823</td>\n      <td>0.031018</td>\n      <td>0.074629</td>\n      <td>0.010480</td>\n      <td>0.046114</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.114708</td>\n    </tr>\n    <tr>\n      <th>044023722X</th>\n      <td>0.044481</td>\n      <td>0.007245</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.177880</td>\n      <td>0.034955</td>\n      <td>0.005635</td>\n      <td>0.020357</td>\n      <td>1.000000</td>\n      <td>0.107019</td>\n      <td>...</td>\n      <td>0.087744</td>\n      <td>0.049298</td>\n      <td>0.057762</td>\n      <td>0.076484</td>\n      <td>0.017319</td>\n      <td>0.012160</td>\n      <td>0.053510</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033276</td>\n    </tr>\n    <tr>\n      <th>044651652X</th>\n      <td>0.064775</td>\n      <td>0.000000</td>\n      <td>0.034416</td>\n      <td>0.150756</td>\n      <td>0.146524</td>\n      <td>0.197952</td>\n      <td>0.191460</td>\n      <td>0.023057</td>\n      <td>0.107019</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.184257</td>\n      <td>0.078507</td>\n      <td>0.015287</td>\n      <td>0.029424</td>\n      <td>0.061978</td>\n      <td>0.090909</td>\n      <td>0.246183</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 200 columns</p>\n</div>","text/plain":"ISBN        006101351X  014025448X  014028009X  034540288X  038079487X  \\\nISBN                                                                     \n006101351X    1.000000    0.005847    0.044958    0.035806    0.000000   \n014025448X    0.005847    1.000000    0.009320    0.000000    0.044639   \n014028009X    0.044958    0.009320    1.000000    0.000000    0.000000   \n034540288X    0.035806    0.000000    0.000000    1.000000    0.000000   \n038079487X    0.000000    0.044639    0.000000    0.000000    1.000000   \n043935806X    0.051717    0.000000    0.000000    0.032827    0.000000   \n044021145X    0.036379    0.031109    0.032617    0.063500    0.000000   \n044022165X    0.032858    0.000000    0.000000    0.000000    0.055744   \n044023722X    0.044481    0.007245    0.000000    0.000000    0.177880   \n044651652X    0.064775    0.000000    0.034416    0.150756    0.146524   \n\nISBN        043935806X  044021145X  044022165X  044023722X  044651652X  ...  \\\nISBN                                                                    ...   \n006101351X    0.051717    0.036379    0.032858    0.044481    0.064775  ...   \n014025448X    0.000000    0.031109    0.000000    0.007245    0.000000  ...   \n014028009X    0.000000    0.032617    0.000000    0.000000    0.034416  ...   \n034540288X    0.032827    0.063500    0.000000    0.000000    0.150756  ...   \n038079487X    0.000000    0.000000    0.055744    0.177880    0.146524  ...   \n043935806X    1.000000    0.041690    0.005021    0.034955    0.197952  ...   \n044021145X    0.041690    1.000000    0.019424    0.005635    0.191460  ...   \n044022165X    0.005021    0.019424    1.000000    0.020357    0.023057  ...   \n044023722X    0.034955    0.005635    0.020357    1.000000    0.107019  ...   \n044651652X    0.197952    0.191460    0.023057    0.107019    1.000000  ...   \n\nISBN        743418174  767902521  767905180  786868716  786881852  804106304  \\\nISBN                                                                           \n006101351X   0.032187   0.047741   0.009323   0.007262   0.006989   0.058881   \n014025448X   0.033029   0.049897   0.000000   0.000000   0.000000   0.000000   \n014028009X   0.030783   0.006341   0.000000   0.000000   0.000000   0.391058   \n034540288X   0.044947   0.000000   0.000000   0.025351   0.000000   0.000000   \n038079487X   0.051876   0.033748   0.063267   0.024639   0.000000   0.049947   \n043935806X   0.014754   0.328266   0.000000   0.003329   0.032035   0.000000   \n044021145X   0.031395   0.038806   0.066136   0.003220   0.006197   0.000000   \n044022165X   0.175295   0.000000   0.039823   0.031018   0.074629   0.010480   \n044023722X   0.087744   0.049298   0.057762   0.076484   0.017319   0.012160   \n044651652X   0.000000   0.184257   0.078507   0.015287   0.029424   0.061978   \n\nISBN        804114986  805063897  842329129  971880107  \nISBN                                                    \n006101351X   0.043183   0.029235   0.006989   0.026854  \n014025448X   0.000000   0.000000   0.071714   0.010206  \n014028009X   0.189290   0.279600   0.000000   0.007134  \n034540288X   0.000000   0.000000   0.048795   0.000000  \n038079487X   0.000000   0.000000   0.539464   0.000000  \n043935806X   0.000000   0.000000   0.006407   0.131306  \n044021145X   0.191460   0.000000   0.037182   0.007938  \n044022165X   0.046114   0.000000   0.000000   0.114708  \n044023722X   0.053510   0.000000   0.000000   0.033276  \n044651652X   0.090909   0.246183   0.000000   0.000000  \n\n[10 rows x 200 columns]"},"exec_count":18}},"pos":23,"start":1610068384951,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068385377,"exec_count":19,"id":"55a0a4","input":"#Item-Based Collaborative Filter using Weighted Mean Ratings\ndef cf_item_wmean(user_id, book, ratings_matrix, c_sim_matrix):\n    \n    #Check if user exists in r_matrix\n    if user_id in ratings_matrix:\n        \n        #Get the similarity scores for the item in question with every other item\n        sim_scores = c_sim_matrix[book]\n        \n        #Get the user ratings for the book in question\n        m_ratings = ratings_matrix[user_id]\n        \n        #Extract the indices containing NaN in the m_ratings series\n        idx = m_ratings[m_ratings.isnull()].index\n        \n        #Drop the NaN values from the m_ratings Series\n        m_ratings = m_ratings.dropna()\n        \n        #Drop the corresponding cosine scores from the sim_scores series\n        sim_scores = sim_scores.drop(idx)\n        \n        #Compute the final weighted mean\n        if sim_scores.sum() > 0:\n            wmean_rating = np.dot(sim_scores, m_ratings)/ sim_scores.sum()\n        else: # the book has zero cosine similarity with other books\n            wmean_rating = 6\n    \n    else:\n        #Default to a rating of 6.0 in the absence of any information\n        wmean_rating = 6.0\n    \n    return wmean_rating","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"pos":24,"start":1610068385360,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068386602,"exec_count":20,"id":"9c80d3","input":"score(cf_item_wmean, X_test, r_matrix_item, cosine_sim_item)","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"output":{"0":{"data":{"text/plain":"3.4119539180908327"},"exec_count":20}},"pos":25,"start":1610068385381,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068386635,"exec_count":21,"id":"e534b6","input":"bx.head()","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User-ID</th>\n      <th>ISBN</th>\n      <th>Book-Rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6251</td>\n      <td>345370775</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6251</td>\n      <td>044021145X</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6251</td>\n      <td>312983271</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6251</td>\n      <td>080410526X</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6251</td>\n      <td>743418174</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   User-ID        ISBN  Book-Rating\n0     6251   345370775            1\n1     6251  044021145X            1\n2     6251   312983271            1\n3     6251  080410526X            1\n4     6251   743418174            1"},"exec_count":21}},"pos":28,"start":1610068386615,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068386907,"exec_count":22,"id":"dd57d2","input":"#Import the required classes and methods from the surprise library\nfrom surprise import Reader, Dataset, KNNBasic\n\n#Define a Reader object\n#The Reader object helps in parsing the file or dataframe containing ratings\nreader = Reader(rating_scale=(1,11))\n\n#Create the dataset to be used for building the filter\n#data = Dataset.load_from_df(ratings, reader)\ndata = Dataset.load_from_df(bx, reader)\n\n#Define the algorithm object; in this case kNN\nnp.random.seed(1)\nknn = KNNBasic(verbose=False)\n\n#Replace \"evaluate\" with \"cross_validate\"\n#Evaluate the performance in terms of RMSE\nfrom surprise.model_selection import cross_validate\nknn_cv = cross_validate(knn, data, measures=['RMSE'], cv=5, verbose=True)\n#to extract the mean RMSE, we need to get the mean of the test_rmse values\nknn_RMSE = np.mean(knn_cv['test_rmse'])\nprint(f'\\nThe RMSE across five folds was {knn_RMSE}')","kernel":"python3","metadata":{"hidden":true},"no_halt":true,"output":{"0":{"name":"stdout","text":"Evaluating RMSE of algorithm KNNBasic on 5 split(s).\n\n                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \nRMSE (testset)    3.6110  3.6344  3.7526  3.6892  3.3253  3.6025  0.1470  \nFit time          0.00    0.00    0.00    0.00    0.00    0.00    0.00    \nTest time         0.02    0.02    0.01    0.01    0.02    0.02    0.00    \n\nThe RMSE across five folds was 3.602520575669625\n"}},"pos":29,"scrolled":true,"start":1610068386644,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068387902,"exec_count":23,"id":"5a6d0f","input":"#Import SVD\nfrom surprise import SVD\n\n#Define a Reader object\n#The Reader object helps in parsing the file or dataframe containing ratings\nreader = Reader(rating_scale=(1,11))\n\n#Create the dataset to be used for building the filter\n\n#define a random seed for consistent results\nnp.random.seed(1)\ndata = Dataset.load_from_df(bx, reader)\n\n#Define the SVD algorithm object\nsvd = SVD()\n\n#Evaluate the performance in terms of RMSE\nsvd_cv = cross_validate(svd, data, measures=['RMSE'], cv=5, verbose=True)\n#to extract the mean RMSE, we need to get the mean of the test_rmse values\nsvd_RMSE = np.mean(svd_cv['test_rmse'])\nprint(f'\\nThe RMSE across five folds was {svd_RMSE}')","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"Evaluating RMSE of algorithm SVD on 5 split(s).\n\n                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \nRMSE (testset)    3.3147  3.2801  3.3752  3.3300  2.9999  3.2600  0.1336  \nFit time          0.20    0.16    0.19    0.15    0.15    0.17    0.02    \nTest time         0.00    0.01    0.00    0.00    0.00    0.00    0.00    \n\nThe RMSE across five folds was 3.2599747237616166\n"}},"pos":32,"start":1610068386921,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068387915,"exec_count":24,"id":"f2ad9a","input":"# load the data\nimport pandas as pd\nimport numpy as np\nbx = pd.read_csv('./data/BX-Book-Ratings-3000.csv')","kernel":"python3","no_halt":true,"pos":35,"start":1610068387907,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068387961,"exec_count":25,"id":"afe5d1","input":"#Build the ratings matrix using pivot_table function\n#r_matrix = X_train.pivot_table(values='Book-Rating', index='ISBN', columns='User-ID')\nr_matrix_item = bx.pivot(values='Book-Rating', index='ISBN', columns='User-ID')\n\nr_matrix_item.head()","kernel":"python3","no_halt":true,"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>User-ID</th>\n      <th>6251</th>\n      <th>6575</th>\n      <th>7346</th>\n      <th>11601</th>\n      <th>11676</th>\n      <th>13552</th>\n      <th>14521</th>\n      <th>16795</th>\n      <th>21014</th>\n      <th>23768</th>\n      <th>...</th>\n      <th>238781</th>\n      <th>254465</th>\n      <th>258534</th>\n      <th>260897</th>\n      <th>261829</th>\n      <th>265115</th>\n      <th>265313</th>\n      <th>266226</th>\n      <th>269566</th>\n      <th>274308</th>\n    </tr>\n    <tr>\n      <th>ISBN</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>006101351X</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>9.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>014025448X</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>014028009X</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>034540288X</th>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>038079487X</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 100 columns</p>\n</div>","text/plain":"User-ID     6251    6575    7346    11601   11676   13552   14521   16795   \\\nISBN                                                                         \n006101351X     NaN     NaN     1.0     NaN     9.0     NaN     6.0     NaN   \n014025448X     NaN     NaN     NaN     1.0     NaN     NaN     NaN     NaN   \n014028009X     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n034540288X     1.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n038079487X     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n\nUser-ID     21014   23768   ...  238781  254465  258534  260897  261829  \\\nISBN                        ...                                           \n006101351X     NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n014025448X     1.0     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n014028009X     NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n034540288X     NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n038079487X     NaN     1.0  ...     NaN     NaN     NaN     NaN     NaN   \n\nUser-ID     265115  265313  266226  269566  274308  \nISBN                                                \n006101351X     NaN     NaN     1.0     NaN     1.0  \n014025448X     8.0     NaN     NaN     NaN     NaN  \n014028009X     8.0     NaN     NaN     NaN     NaN  \n034540288X     NaN     NaN     NaN     NaN     1.0  \n038079487X     1.0     NaN     NaN     NaN     NaN  \n\n[5 rows x 100 columns]"},"exec_count":25}},"pos":36,"start":1610068387936,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068387971,"exec_count":26,"id":"bb78f1","input":"#Create a dummy ratings matrix with all null values imputed to 0\nr_matrix_item_dummy = r_matrix_item.copy().fillna(0)","kernel":"python3","no_halt":true,"pos":37,"start":1610068387968,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068388072,"exec_count":27,"id":"9bd53a","input":"# Import cosine_score \nfrom sklearn.metrics.pairwise import cosine_similarity\n\n#Compute the cosine similarity matrix using the dummy ratings matrix\ncosine_sim_item = cosine_similarity(r_matrix_item_dummy, r_matrix_item_dummy)","kernel":"python3","no_halt":true,"pos":38,"start":1610068387976,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068388287,"exec_count":28,"id":"9dd133","input":"#Convert into pandas dataframe \ncosine_sim_item = pd.DataFrame(cosine_sim_item, index=r_matrix_item.index, columns=r_matrix_item.index)\n\ncosine_sim_item.head(10)","kernel":"python3","no_halt":true,"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>ISBN</th>\n      <th>006101351X</th>\n      <th>014025448X</th>\n      <th>014028009X</th>\n      <th>034540288X</th>\n      <th>038079487X</th>\n      <th>043935806X</th>\n      <th>044021145X</th>\n      <th>044022165X</th>\n      <th>044023722X</th>\n      <th>044651652X</th>\n      <th>...</th>\n      <th>743418174</th>\n      <th>767902521</th>\n      <th>767905180</th>\n      <th>786868716</th>\n      <th>786881852</th>\n      <th>804106304</th>\n      <th>804114986</th>\n      <th>805063897</th>\n      <th>842329129</th>\n      <th>971880107</th>\n    </tr>\n    <tr>\n      <th>ISBN</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>006101351X</th>\n      <td>1.000000</td>\n      <td>0.009040</td>\n      <td>0.411609</td>\n      <td>0.068252</td>\n      <td>0.316978</td>\n      <td>0.065070</td>\n      <td>0.021290</td>\n      <td>0.061752</td>\n      <td>0.043490</td>\n      <td>0.212790</td>\n      <td>...</td>\n      <td>0.041756</td>\n      <td>0.230627</td>\n      <td>0.004612</td>\n      <td>0.036638</td>\n      <td>0.003855</td>\n      <td>0.370128</td>\n      <td>0.216256</td>\n      <td>0.046480</td>\n      <td>0.005387</td>\n      <td>0.026835</td>\n    </tr>\n    <tr>\n      <th>014025448X</th>\n      <td>0.009040</td>\n      <td>1.000000</td>\n      <td>0.240276</td>\n      <td>0.000000</td>\n      <td>0.037747</td>\n      <td>0.000000</td>\n      <td>0.024101</td>\n      <td>0.000000</td>\n      <td>0.007033</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.031987</td>\n      <td>0.364547</td>\n      <td>0.000000</td>\n      <td>0.004102</td>\n      <td>0.056105</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.070571</td>\n      <td>0.097646</td>\n    </tr>\n    <tr>\n      <th>014028009X</th>\n      <td>0.411609</td>\n      <td>0.240276</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.220576</td>\n      <td>0.002096</td>\n      <td>0.199513</td>\n      <td>0.002304</td>\n      <td>0.000000</td>\n      <td>0.175953</td>\n      <td>...</td>\n      <td>0.045171</td>\n      <td>0.216187</td>\n      <td>0.029420</td>\n      <td>0.033957</td>\n      <td>0.002732</td>\n      <td>0.220911</td>\n      <td>0.114955</td>\n      <td>0.094712</td>\n      <td>0.030547</td>\n      <td>0.009510</td>\n    </tr>\n    <tr>\n      <th>034540288X</th>\n      <td>0.068252</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.021668</td>\n      <td>0.034658</td>\n      <td>0.047619</td>\n      <td>0.070799</td>\n      <td>0.028868</td>\n      <td>...</td>\n      <td>0.048299</td>\n      <td>0.000000</td>\n      <td>0.033787</td>\n      <td>0.020646</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.039467</td>\n      <td>0.098295</td>\n    </tr>\n    <tr>\n      <th>038079487X</th>\n      <td>0.316978</td>\n      <td>0.037747</td>\n      <td>0.220576</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.002744</td>\n      <td>0.002195</td>\n      <td>0.057299</td>\n      <td>0.188316</td>\n      <td>0.032907</td>\n      <td>...</td>\n      <td>0.055058</td>\n      <td>0.023010</td>\n      <td>0.034236</td>\n      <td>0.047070</td>\n      <td>0.039344</td>\n      <td>0.294372</td>\n      <td>0.137953</td>\n      <td>0.000000</td>\n      <td>0.459897</td>\n      <td>0.006225</td>\n    </tr>\n    <tr>\n      <th>043935806X</th>\n      <td>0.065070</td>\n      <td>0.000000</td>\n      <td>0.002096</td>\n      <td>0.021668</td>\n      <td>0.002744</td>\n      <td>1.000000</td>\n      <td>0.022529</td>\n      <td>0.003095</td>\n      <td>0.032215</td>\n      <td>0.041282</td>\n      <td>...</td>\n      <td>0.012558</td>\n      <td>0.212561</td>\n      <td>0.000000</td>\n      <td>0.032209</td>\n      <td>0.022027</td>\n      <td>0.002650</td>\n      <td>0.064362</td>\n      <td>0.011067</td>\n      <td>0.005131</td>\n      <td>0.178904</td>\n    </tr>\n    <tr>\n      <th>044021145X</th>\n      <td>0.021290</td>\n      <td>0.024101</td>\n      <td>0.199513</td>\n      <td>0.034658</td>\n      <td>0.002195</td>\n      <td>0.022529</td>\n      <td>1.000000</td>\n      <td>0.032183</td>\n      <td>0.051529</td>\n      <td>0.033017</td>\n      <td>...</td>\n      <td>0.053567</td>\n      <td>0.020778</td>\n      <td>0.059721</td>\n      <td>0.023613</td>\n      <td>0.008808</td>\n      <td>0.027557</td>\n      <td>0.123541</td>\n      <td>0.053106</td>\n      <td>0.024621</td>\n      <td>0.332156</td>\n    </tr>\n    <tr>\n      <th>044022165X</th>\n      <td>0.061752</td>\n      <td>0.000000</td>\n      <td>0.002304</td>\n      <td>0.047619</td>\n      <td>0.057299</td>\n      <td>0.003095</td>\n      <td>0.032183</td>\n      <td>1.000000</td>\n      <td>0.065742</td>\n      <td>0.045363</td>\n      <td>...</td>\n      <td>0.370294</td>\n      <td>0.000000</td>\n      <td>0.024133</td>\n      <td>0.289044</td>\n      <td>0.044376</td>\n      <td>0.020387</td>\n      <td>0.056580</td>\n      <td>0.012161</td>\n      <td>0.000000</td>\n      <td>0.126379</td>\n    </tr>\n    <tr>\n      <th>044023722X</th>\n      <td>0.043490</td>\n      <td>0.007033</td>\n      <td>0.000000</td>\n      <td>0.070799</td>\n      <td>0.188316</td>\n      <td>0.032215</td>\n      <td>0.051529</td>\n      <td>0.065742</td>\n      <td>1.000000</td>\n      <td>0.036788</td>\n      <td>...</td>\n      <td>0.136782</td>\n      <td>0.038586</td>\n      <td>0.086115</td>\n      <td>0.074548</td>\n      <td>0.071975</td>\n      <td>0.008660</td>\n      <td>0.084122</td>\n      <td>0.009040</td>\n      <td>0.092209</td>\n      <td>0.041755</td>\n    </tr>\n    <tr>\n      <th>044651652X</th>\n      <td>0.212790</td>\n      <td>0.000000</td>\n      <td>0.175953</td>\n      <td>0.028868</td>\n      <td>0.032907</td>\n      <td>0.041282</td>\n      <td>0.033017</td>\n      <td>0.045363</td>\n      <td>0.036788</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.089234</td>\n      <td>0.069225</td>\n      <td>0.017556</td>\n      <td>0.003576</td>\n      <td>0.014673</td>\n      <td>0.116527</td>\n      <td>0.068599</td>\n      <td>0.014744</td>\n      <td>0.000000</td>\n      <td>0.008513</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 200 columns</p>\n</div>","text/plain":"ISBN        006101351X  014025448X  014028009X  034540288X  038079487X  \\\nISBN                                                                     \n006101351X    1.000000    0.009040    0.411609    0.068252    0.316978   \n014025448X    0.009040    1.000000    0.240276    0.000000    0.037747   \n014028009X    0.411609    0.240276    1.000000    0.000000    0.220576   \n034540288X    0.068252    0.000000    0.000000    1.000000    0.000000   \n038079487X    0.316978    0.037747    0.220576    0.000000    1.000000   \n043935806X    0.065070    0.000000    0.002096    0.021668    0.002744   \n044021145X    0.021290    0.024101    0.199513    0.034658    0.002195   \n044022165X    0.061752    0.000000    0.002304    0.047619    0.057299   \n044023722X    0.043490    0.007033    0.000000    0.070799    0.188316   \n044651652X    0.212790    0.000000    0.175953    0.028868    0.032907   \n\nISBN        043935806X  044021145X  044022165X  044023722X  044651652X  ...  \\\nISBN                                                                    ...   \n006101351X    0.065070    0.021290    0.061752    0.043490    0.212790  ...   \n014025448X    0.000000    0.024101    0.000000    0.007033    0.000000  ...   \n014028009X    0.002096    0.199513    0.002304    0.000000    0.175953  ...   \n034540288X    0.021668    0.034658    0.047619    0.070799    0.028868  ...   \n038079487X    0.002744    0.002195    0.057299    0.188316    0.032907  ...   \n043935806X    1.000000    0.022529    0.003095    0.032215    0.041282  ...   \n044021145X    0.022529    1.000000    0.032183    0.051529    0.033017  ...   \n044022165X    0.003095    0.032183    1.000000    0.065742    0.045363  ...   \n044023722X    0.032215    0.051529    0.065742    1.000000    0.036788  ...   \n044651652X    0.041282    0.033017    0.045363    0.036788    1.000000  ...   \n\nISBN        743418174  767902521  767905180  786868716  786881852  804106304  \\\nISBN                                                                           \n006101351X   0.041756   0.230627   0.004612   0.036638   0.003855   0.370128   \n014025448X   0.031987   0.364547   0.000000   0.004102   0.056105   0.000000   \n014028009X   0.045171   0.216187   0.029420   0.033957   0.002732   0.220911   \n034540288X   0.048299   0.000000   0.033787   0.020646   0.000000   0.000000   \n038079487X   0.055058   0.023010   0.034236   0.047070   0.039344   0.294372   \n043935806X   0.012558   0.212561   0.000000   0.032209   0.022027   0.002650   \n044021145X   0.053567   0.020778   0.059721   0.023613   0.008808   0.027557   \n044022165X   0.370294   0.000000   0.024133   0.289044   0.044376   0.020387   \n044023722X   0.136782   0.038586   0.086115   0.074548   0.071975   0.008660   \n044651652X   0.089234   0.069225   0.017556   0.003576   0.014673   0.116527   \n\nISBN        804114986  805063897  842329129  971880107  \nISBN                                                    \n006101351X   0.216256   0.046480   0.005387   0.026835  \n014025448X   0.000000   0.000000   0.070571   0.097646  \n014028009X   0.114955   0.094712   0.030547   0.009510  \n034540288X   0.000000   0.000000   0.039467   0.098295  \n038079487X   0.137953   0.000000   0.459897   0.006225  \n043935806X   0.064362   0.011067   0.005131   0.178904  \n044021145X   0.123541   0.053106   0.024621   0.332156  \n044022165X   0.056580   0.012161   0.000000   0.126379  \n044023722X   0.084122   0.009040   0.092209   0.041755  \n044651652X   0.068599   0.014744   0.000000   0.008513  \n\n[10 rows x 200 columns]"},"exec_count":28}},"pos":39,"start":1610068388096,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068389154,"exec_count":29,"id":"c0ba8d","input":"#Build the SVD based Collaborative filter\nfrom surprise import SVD, Reader, Dataset\nfrom surprise.model_selection import cross_validate\n\nreader = Reader(rating_scale=(1,11))\nratings = pd.read_csv('./data/BX-Book-Ratings-3000.csv')\ndata = Dataset.load_from_df(ratings[['User-ID', 'ISBN', 'Book-Rating']], reader)\n\nalgo = SVD()\n\ncross_validate(algo,data,cv=5,verbose=True)","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n\n                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \nRMSE (testset)    3.2839  3.2812  3.1817  3.1385  3.2739  3.2318  0.0602  \nMAE (testset)     2.3938  2.3721  2.3547  2.3184  2.3781  2.3634  0.0258  \nFit time          0.14    0.14    0.17    0.13    0.14    0.14    0.01    \nTest time         0.00    0.00    0.00    0.00    0.01    0.00    0.00    \n"},"1":{"data":{"text/plain":"{'test_rmse': array([3.28388376, 3.28117795, 3.18166757, 3.1385093 , 3.27388535]),\n 'test_mae': array([2.39375786, 2.37208805, 2.35465903, 2.31836337, 2.37811165]),\n 'fit_time': (0.13773775100708008,\n  0.1397688388824463,\n  0.16584992408752441,\n  0.13236308097839355,\n  0.14080071449279785),\n 'test_time': (0.003816843032836914,\n  0.004055976867675781,\n  0.0037698745727539062,\n  0.0037529468536376953,\n  0.0054149627685546875)}"},"exec_count":29}},"pos":41,"start":1610068388294,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068389167,"exec_count":30,"id":"bc8255","input":"#take in the user_id, the isbn number, a cosine matrix of how books are similar with books, \n#the data that you're going to use for predicting ratings, and the predictor.\n\ndef hybrid(user_id, isbn, cosine_matrix, data, predictor):\n    \n    #get column index from column name in the cosine_matrix\n    idx = cosine_matrix.columns.get_loc(isbn) \n    \n    #Extract the similarity scores and their corresponding index for every item from the cosine_sim matrix\n    sim_scores = list(enumerate(cosine_sim_item[isbn]))\n    \n    #excluding the similarity score of the item with itself\n    del sim_scores[idx]\n    \n    #Sort the (index, score) tuples in decreasing order of similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    \n    #Select the top 25 tuples\n    sim_scores = sim_scores[0:25]\n    \n    #Store the cosine_sim indices of the top 25 items in a list\n    item_indices = [i[0] for i in sim_scores]\n\n    #Extract the metadata of the aforementioned items\n    items = data.iloc[item_indices][['ISBN']]\n    \n    #Compute the predicted ratings using the SVD filter\n    items['est_rating'] = items['ISBN'].apply(lambda x: predictor.predict(user_id, x).est)\n    \n    #Sort the items in decreasing order of predicted rating\n    items = items.sort_values('est_rating', ascending=False)\n    \n    #Return the top 10 items as recommendations\n    return items.head(10)","kernel":"python3","no_halt":true,"pos":42,"start":1610068389163,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068389194,"exec_count":31,"id":"b767f8","input":"hybrid('31315', '440214041', cosine_sim_item, bx, algo)","kernel":"python3","no_halt":true,"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ISBN</th>\n      <th>est_rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>151</th>\n      <td>385504209</td>\n      <td>4.157843</td>\n    </tr>\n    <tr>\n      <th>107</th>\n      <td>60915544</td>\n      <td>4.044923</td>\n    </tr>\n    <tr>\n      <th>181</th>\n      <td>440213525</td>\n      <td>3.975238</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>786868716</td>\n      <td>3.345905</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>60938455</td>\n      <td>3.125730</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>671027360</td>\n      <td>3.013788</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>068484477X</td>\n      <td>2.863689</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>385484518</td>\n      <td>2.742230</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>044022165X</td>\n      <td>2.735795</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>553279912</td>\n      <td>2.704312</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"           ISBN  est_rating\n151   385504209    4.157843\n107    60915544    4.044923\n181   440213525    3.975238\n28    786868716    3.345905\n130    60938455    3.125730\n177   671027360    3.013788\n126  068484477X    2.863689\n45    385484518    2.742230\n175  044022165X    2.735795\n97    553279912    2.704312"},"exec_count":31}},"pos":43,"start":1610068389172,"state":"done","type":"cell"}
{"cell_type":"code","end":1610068389316,"exec_count":32,"id":"66c844","input":"#first train the Knn\nknn_predictor =KNNBasic(k = 5, verbose=False)\ncross_validate(knn_predictor,data,cv=5,verbose=True)\nhybrid('31315', '440214041', cosine_sim_item, bx, knn_predictor)","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"Evaluating RMSE, MAE of algorithm KNNBasic on 5 split(s).\n\n                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \nRMSE (testset)    3.7302  3.7440  3.5808  3.7720  3.3801  3.6414  0.1466  \nMAE (testset)     2.4358  2.4619  2.3585  2.4617  2.1171  2.3670  0.1305  \nFit time          0.00    0.00    0.00    0.00    0.00    0.00    0.00    \nTest time         0.02    0.01    0.01    0.01    0.01    0.01    0.00    \n"},"1":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ISBN</th>\n      <th>est_rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>119</th>\n      <td>671001795</td>\n      <td>2.6875</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>380002930</td>\n      <td>2.6875</td>\n    </tr>\n    <tr>\n      <th>111</th>\n      <td>671021001</td>\n      <td>2.6875</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>553280341</td>\n      <td>2.6875</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>316284955</td>\n      <td>2.6875</td>\n    </tr>\n    <tr>\n      <th>152</th>\n      <td>440214041</td>\n      <td>2.6875</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>385504209</td>\n      <td>2.6875</td>\n    </tr>\n    <tr>\n      <th>107</th>\n      <td>60915544</td>\n      <td>2.6875</td>\n    </tr>\n    <tr>\n      <th>153</th>\n      <td>446611867</td>\n      <td>2.6875</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>671027360</td>\n      <td>2.6875</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"          ISBN  est_rating\n119  671001795      2.6875\n55   380002930      2.6875\n111  671021001      2.6875\n104  553280341      2.6875\n25   316284955      2.6875\n152  440214041      2.6875\n151  385504209      2.6875\n107   60915544      2.6875\n153  446611867      2.6875\n177  671027360      2.6875"},"exec_count":32}},"pos":45,"start":1610068389201,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"4d1c47","input":"# *Self-Assessment: Weighted Mean Item-Based Filter - Solution*","metadata":{"heading_collapsed":true},"pos":19,"type":"cell"}
{"cell_type":"markdown","id":"596fd7","input":"# *Self-Assessment: SVD Filter - Solution*","pos":31,"type":"cell"}
{"cell_type":"markdown","id":"7b3c0b","input":"The RMSE for each model used so far are stated below ranked from best to worst:\n\n- weighted-mean item-based collaborative filter: RMSE = 3.41 \n\n- kNN-based collaborative filter: (average) RMSE = 3.49 (note that this one will vary slightly if you didn't set a seed or if you use a different seed)\n\n- weighted-mean item-based collaborative filter: RMSE = 3.61 \n\n- baseline model: RMSE = 4.70.  ","metadata":{"hidden":true},"pos":30,"type":"cell"}
{"cell_type":"markdown","id":"7cbc54","input":"The RMSE for each model used so far are stated below ranked from best to worst:\n\n- SVD filter: (average) RMSE = 3.25 (note that this one will vary slightly if you don't set a seed or use a different seed)\n\n- weighted-mean item-based collaborative filter: RMSE = 3.41 \n\n- kNN-based collaborative filter: (average) RMSE = 3.49 (note that this one will vary slightly since it is based on randomly selected subsets in cross-validation)\n\n- weighted-mean item-based collaborative filter: RMSE = 3.61 \n\n- baseline model: RMSE = 4.70.  ","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"8f5996","input":"# *Self-Assessment: Type of Recommenders - Solution*","pos":46,"type":"cell"}
{"cell_type":"markdown","id":"91044d","input":"Note that the surprise package changed a bit since the book by Banik was published.  The code to train a model using cross validation has changed a bit as shown below.  Also, notice that we aren't splitting the data into test and training and testing sets, rather we're using the whole dataset for illustration.","pos":40,"type":"cell"}
{"cell_type":"markdown","id":"a22bd4","input":"The weighted-mean item-based collaborative filter is the best so far at RMSE = 3.41.  The weighted-mean item-based collaborative filter had RMSE = 3.61 and the baseline model had RMSE = 4.70.  ","metadata":{"hidden":true},"pos":26,"type":"cell"}
{"cell_type":"markdown","id":"a2ba2d","input":"# *Self-Assessment: Hybrid Recommender*","pos":34,"type":"cell"}
{"cell_type":"markdown","id":"a468cd","input":"# *Self-Assessment: kNN-Based Collaborative Filter - Solution*","metadata":{"heading_collapsed":true},"pos":27,"type":"cell"}
{"cell_type":"markdown","id":"b8a6d2","input":"# *Self-Assessment: Setting up the File*","metadata":{"heading_collapsed":true},"pos":2,"type":"cell"}
{"cell_type":"markdown","id":"c26a39","input":"**Put the letter of the recommender system with the number of its description.**\n\n1. c\n\n2. e\n\n3. a\n\n4. f\n\n5. b\n\n6. d\n","pos":47,"type":"cell"}
{"cell_type":"markdown","id":"c68129","input":"The RMSE with the user-based collaborative filter is 3.61 compared to 4.70 for the baseline model, so predicted ratings are more precise.  ","metadata":{"hidden":true},"pos":18,"type":"cell"}
{"cell_type":"markdown","id":"c9552e","input":"# *Self-Assessment: Baseline RMSE to Assess Model Performance*","metadata":{"heading_collapsed":true},"pos":6,"type":"cell"}
{"cell_type":"markdown","id":"e7a59e","input":"### Bonus\nSince both SVD and KNN function the same way from a technical standpoint, we can train a KNN model and pass it in to our same hybrid function to see what kinds of predictions we get from that.","pos":44,"type":"cell"}
{"cell_type":"markdown","id":"fb19d8","input":"# *Self-Assessment: Weighted Mean User-Based Filter*","metadata":{"heading_collapsed":true},"pos":10,"type":"cell"}
{"cell_type":"markdown","id":"fe7788","input":"<font size=18>Lesson 11 - Self-Assessment Solutions</font>","pos":1,"type":"cell"}
{"id":0,"time":1610068194679,"type":"user"}
{"last_load":1610068194965,"type":"file"}