{"backend_state":"init","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"trust":false,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"5a8970","input":"","pos":16,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"80e3d3","input":"","pos":20,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"a39278","input":"","metadata":{"hidden":true},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"960cf3","input":"# Do not change this cell for loading and preparing the data\nimport pandas as pd\nimport numpy as np\n\nX = pd.read_csv('./data/loans_subset.csv')\n\n# split into predictors and target\n# convert to numpy arrays for xgboost, OK for other models too\ny = np.array(X['status_Bad']) # 1 for bad loan, 0 for good loan\nX = np.array(X.drop(columns = ['status_Bad']))\n\n# split into test and training data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)","pos":10,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"3bf624","input":"# check the code in Part 3 to see how to get the confusion matrix to help you write your function","pos":12,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"8de746","input":"# We've included this code to get you started\n\nfrom sklearn.linear_model import LogisticRegression\n\n# we do need to go higher than the default iterations for the solver to get convergence\n# and the explicity declaration of the solver avoids a warning message, otherwise\n# the parameters are defaults.\nlogreg_model = LogisticRegression(solver='lbfgs',max_iter=1000)\n\nlogreg_model.fit(X_train, y_train)\n\n# Use score method to get accuracy of model\nscore = logreg_model.score(X_test, y_test) # this is accuracy\nprint(score)\n\n# obtaining the confusion matrix and making it look nice\n\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\n\ny_pred = logreg_model.predict(X_test)\n\n# must put true before predictions in confusion matrix function\ncmtx = pd.DataFrame(\n    \n    confusion_matrix(y_test, y_pred, labels=[1,0]), \n    index=['true:bad', 'true:good'], \n    columns=['pred:bad','pred:good']\n)\ndisplay(cmtx)","output":{"0":{"name":"stdout","output_type":"stream","text":"0.5475\n"},"1":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pred:bad</th>\n      <th>pred:good</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>true:bad</th>\n      <td>126</td>\n      <td>71</td>\n    </tr>\n    <tr>\n      <th>true:good</th>\n      <td>110</td>\n      <td>93</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"           pred:bad  pred:good\ntrue:bad        126         71\ntrue:good       110         93"},"exec_count":9,"output_type":"execute_result"}},"pos":14,"type":"cell"}
{"cell_type":"markdown","id":"0ee363","input":"### Summary:\n<font color = \"blue\"> *** 5 points: </font>","metadata":{"hidden":true},"pos":5,"type":"cell"}
{"cell_type":"markdown","id":"178f4c","input":"# **P2.3** - Extra Credit - Problem 3 (up to 10 extra points)\n\nShow how to use the `pycaret` package to do model selection for one of the two problems above.  We've never used `pycaret` but it looks promising.  We have used the `caret` package in R and it simplifies many machine learning tasks considerably.\n\nUse Google to search for `pycaret` to get started.","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"1b6adf","input":"# **P2.2** - Optimize XGBoost Classifier","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"205207","input":"<font color = \"green\">\nreplace this text with answer, your answer should be green   \n</font>","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"331280","input":"### Find optimized hyperparameters for an xgboost classifier model. \n\nThis problem contains 5 parts.\n\n\n### Notes:\n\n#### About the data\nThe first cell below loads a subset of the loans default data from DS705 and your job is to predict whether a loan defaults or not.  The `status_bad` column is the target column and a 1 indicates a loan that defaulted.  We have selected a subset of the original data that includes 2000 each of good and bad loans.  The data has already been cleaned and encoded.  You're welcome to look into a different dataset, but start by getting this working and then add your own data.\n\n#### This is classification, not regression\nThe score for each model will be accuracy and not MSE.  Your summary table should include accuracy, sensitivity, and precision for each optimized model applied to the test data.  (<a href=\"https://classeval.wordpress.com/introduction/basic-evaluation-measures/\">Here is a nice overview of metrics for binary classification data</a>) that includes definitions of accuracy and such.\n\nFor the models you'll mostly just need to change 'regressor' to 'classifier', e.g. `XGBClassifier` instead of `XGBRegressor`.\n\n\nHyperparameter | Type | Default Value | Typical Range\n---- | ---- | ---- | ----\nn_estimators | discrete / integer | 100 | 10 to 150\nmax_depth | discrete / integer | 3| 1 to 10\nmin_child_weight | discrete / integer | 1 | 1 to 20\nlearning_rate | continuous / float | 0.1 | 0.001 to 1\nsubsample | continuous / float | 1 | 0.05 to 1\nreg_lambda | continuous / float | 1 | 0 to 5\nreg_alpha  | continuous / float | 0 | 0 to 5","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"34088e","input":"## **P2.2c** - Baseline Models\n\nStart by training some baseline models using default values of the hyperparameters.  We've included logistic regression in a cell below to get you started.  Use `LogisticRegression`, `RandomForestClassifier`, and `GaussianNB` (Gaussian Naive Bayes) from `sklearn`.  Also use `XGBClassifier` from `xgboost` where you may need to include `objective=\"binary:logistic\"` as an option. The default scoring method for all of the `sklearn` classifiers is accuracy. Apply `my_classifier_results` to the test data for each model.\n\n<font color = \"blue\"> *** 10 points - (don't delete this cell) </font>","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"39de4d","input":"## **P2.2b** - Display Results Function\n\nWrite a function called `my_classifier_results` modeled after `my_regression_results` that applies a model to the test data and prints out the accuracy, sensitivity, precision, and the confusion matrix.  There is no need to make a plot.\n\n<font color = \"blue\"> *** 5 points - (don't delete this cell) </font>","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"52935b","input":"# Project 2 Homework","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"60cc59","input":"## **P2.2e** - Summary\n\n* In addition to your summary table, answer:\n    * The bank isn't as concerned about misclassifying some truly good loans as they are interested in correctly predicting truly bad loans.  Which model should they use?  Why?\n\n<font color = \"blue\"> *** 5 points - (don't delete this cell) </font>","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"67bc89","input":"For this project you're going to apply hyperparameter optimization to both a regression and a classification problem. It looks like a lot to do below, but it's mostly a matter of modifying code from the presentation. \n\n## Objective\n\nFor each of the models in problems 1 and 2 below, apply the following 4 tuning methods from the presentation: GridSearchCV, RandomSearchCV, BayesianOptimization, and TPOT.\n* **For TPOT**: In Problem 1 do only hyperparameter optimization. In Problem 2 do **both** hyperparameter optimization and also run TPOT and let it choose the model. See the presentation for examples of both.\n\n### What to submit\n\nFor each problem you need to include the following:\n\n1. A pandas table that reports:\n    * The best parameters for each tuning method\n    * The optimized score from the test data\n    * The number of model fits used in the optimization\n2. A brief discussion about which hyperparameter optimization approach worked best\n\n### Notes:\n* **For problem 1**: your pandas table should include the best parameters for each of the 4 tuning methods above.\n* **For problem 2**: your pandas table should include the best parameters for each of the 5 tuning methods (the 4 methods above and the TPOT model search).\n* **For GridSearchCV**: you should include at least 2 or 3 values for each hyperparameter and one of those values should be the default.\n* **For BayesianOptimization**: you'll have to use `int()` or `bool()` to cast the float values of the hyperparameters inside your `cv_score()` function.\n* **For TPOT**: you should use a finer grid than for GridSearchCV, but not more than 10 to 20 possible values for each hyperparameter.  You could lower the number of possible values to keep the search space smaller.\n    * If your code is too slow you can reduce the number of cross-validation folds to 3 and if your dataset is really large you can randomly choose a smaller subset of the rows.\n* Use section headers to label your work.  Your summary / discussion should be more than simply \"XYZ is the best model\", but it also shouldn't be more than a few paragraphs and a table.\n\n\n### Regarding data\n\n* You can use either the specified dataset or you can choose your own.  \n    * If you use your own data it should have at least 500 rows and 10 features.  \n    * If your data has categorical features you'll need \"one hot\" encode it (convert categorical features into multiple binary features).  <a href=\"https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\">Here is a nice tutorial</a>.  For categories with only two values you can remove one of the two hot encoded columns.\n* If you do want to use your own data, we suggest first getting things working with the suggested datasets.  Finding, cleaning, and preparing data can take a lot of time.","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"6f073a","input":"## **P2.2d** - Hyperparameter Optimization\n\nNow use the four hyperparameter optimization techniques on `XGBClassifier` and TPOT general model optimization.  Apply `my_classifer_results` to the test data in each case.\n* Feel free to use 3 folds instead of 5 for cross validation to speed things up. \n* Choose a very small number of iterations, population size, etc. until you're sure things are working correctly, then turn up the numbers.  General TPOT optimization will take a while (fair warning: it took about 30 minutes on my Macbook Pro with generations = 10, population_size=40, and cv=5)  \n* The hyperparameters to consider are the same as they were in the presentation , but here they are again for convenience:\n\nHyperparameter | Default Value | Typical Range\n---- | ---- | ----\nn_estimators | 100 | 10 to 100\nmax_depth | 3 | 1 to 10\nmin_child_weight | 1 | 1 to 20\nlearning_rate | 0.1 | 0.001 to 1\nsub_sample | 1 | 0.05 to 1\nreg_lambda | 1 | 0 to 5\nreg_alpha  | 0 | 0 to 5\n\n<font color = \"blue\"> *** 10 points - (don't delete this cell) </font>","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"78a064","input":"## **P2.2a** - Loading the Data","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"b80c47","input":"### Find optimized hyperparameters for a random forest regression model. \n\nYou may use either the diabetes data used in the presentation or a dataset that you choose.  **You do not need to include the TPOT general search for this problem** (use TPOT to optimize RandomForestRegressor, but don't run TPOT to choose a model). Here are ranges for a subset of the hyperparameters:\n\nHyperparameter |Type | Default Value | Typical Range\n---- | ---- | ---- | ----\nn_estimators | discrete / integer | 100 | 10 to 150\nmax_features | continuous / float | 1.0 | 0.05 to 1.0\nmin_samples_split | discrete / integer | 2 | 2 to 20\nmin_samples_leaf | discrete / integer | 1 | 1 to 20\nbootstrap | discrete / boolean | True | True, False\n\n\nYou can add other hyperparameters to the optimization if you wish.\n<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">Documentation for sklearn RandomForestRegressor</a>\n\n<font color = \"blue\"> *** 15 points: </font>","metadata":{"hidden":true},"pos":3,"type":"cell"}
{"cell_type":"markdown","id":"d03c8e","input":"<font color = \"green\">\nreplace this text with answer, your answer should be green   \n</font>","metadata":{"hidden":true},"pos":6,"type":"cell"}
{"cell_type":"markdown","id":"e2ac8d","input":"# **P2.1** - Optimize Random Forest Regression","metadata":{"heading_collapsed":true},"pos":2,"type":"cell"}
{"id":0,"time":1611078887057,"type":"user"}
{"last_load":1611078887004,"type":"file"}