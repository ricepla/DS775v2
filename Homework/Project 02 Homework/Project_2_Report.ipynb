{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project you're going to apply hyperparameter optimization to both a regression and a classification problem. It looks like a lot to do below, but it's mostly a matter of modifying code from the presentation. \n",
    "\n",
    "Guidelines:\n",
    "\n",
    "* Apply all 4 methods from the presentation: GridSearchCV, RandomSearchCV, BayesianOptimization, and TPOT.\n",
    "    * For GridSearchCV you should include at least 2 or 3 values for each hyperparameter and one of those values should be the default.\n",
    "    * For BayesianOptimization you'll have to use `int()` or `bool()` to cast the float values of the hyperparameters inside your `cross_cv()` function.\n",
    "    * For TPOT you should use a finer grid than for GridSearchCV, but not more than 10 to 20 possible values for each hyperparameter.  You could lower the number of possible values to keep the search space smaller.\n",
    "    * If your code is too slow you can reduce the number of cross-validation folds to 3 and if your dataset is really large you can randomly choose a smaller subset of the rows.\n",
    "* For TPOT do both hyperparameter optimization on the specified model and also run TPOT and let it choose the model.\n",
    "* You can use either the specified dataset or you can choose your own.  \n",
    "    * If you use your own data it should have at least 500 rows and 10 features.  \n",
    "    * If your data has categorical features you'll need \"one hot\" encode it (convert categorical features into multiple binary features).  <a href=\"https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\">Here is a nice tutorial</a>.  For categories with only two values you can remove one of the two hot encoded columns.\n",
    "* For each problem you need to include your narrated code and a summary.  \n",
    "* The summaries should each include \n",
    "    * the results of all 5 approaches (the 4 methods and the TPOT model search)\n",
    "    * a table that reports the optimized score from the test data as well as the number of model fits used in the optimization\n",
    "    * a brief discussion about which hyperparameter optimization approach worked best\n",
    "* If you do want to use your own data, we suggest first getting things working with the suggested datasets.  Finding, cleaning, and preparing data can take a lot of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Optimize Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find optimized hyperparameters for a random forest regression model. You may use either the diabetes data used in the presentation or a dataset that you choose.  **You do not need to include the TPOT general search for this problem**.  Include code and summary.  Here are ranges for a subset of the hyperparameters:\n",
    "\n",
    "Hyperparameter |Type | Default Value | Typical Range\n",
    "---- | ---- | ---- | ----\n",
    "n_estimators | discrete / integer | 100 | 10 to 150\n",
    "max_features | continuous / float | 1.0 | 0.05 to 1.0\n",
    "min_samples_split | discrete / integer | 2 | 2 to 20\n",
    "min_samples_leaf | discrete / integer | 1 | 1 to 20\n",
    "bootstrap | discrete / boolean | True | True, False\n",
    "\n",
    "\n",
    "You can add other hyperparameters to the optimization if you wish.\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">Documentation for sklearn RandomForestRegressor</a>\n",
    "\n",
    "<font color = \"blue\"> *** 20 points - add cells below as needed </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Optimize XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find optimized hyperparameters for an xgboost classifier model.  The first cell below loads a subset of the loans default data from DS705 and your job is to predict whether a loan defaults or not.  The `status_bad` column is the target column and a 1 indicates a loan that defaulted.  We have selected a subset of the original data that includes 2000 each of good and bad loans.  The data has already been cleaned and encoded.  You're welcome to look into a different dataset, but start by getting this working and then add your own data.\n",
    "\n",
    "The score for each model will be accuracy.  Your summary table should include accuracy, sensitivity, and precision for each optimized model applied to the test data.  (<a href=\"https://classeval.wordpress.com/introduction/basic-evaluation-measures/\">Here is a nice overview of metrics for binary classification data</a>) that includes definitions of accuracy and such.\n",
    "\n",
    "For the models you'll mostly just need to change 'regressor' to 'classifier', e.g. `XGBClassifier` instead of `XGBRegressor`.\n",
    "\n",
    "Here is a basic outline of what to do:\n",
    "\n",
    "* Write a function called `my_classifier_results` modeled after `my_regression_results` that applies a model to the test data and prints out the accuracy, sensitivity, precision, and the confusion matrix.  There is some code below to help you get started.  There is no need to make a plot.\n",
    "\n",
    "* Start by training some baseline models using default values of the hyperparameters.  We've included logistic regression in a cell below to get you started.  Use `LogisticRegression`, `RandomForestClassifier`, and `GaussianNB` (Gaussian Naive Bayes) from `sklearn`.  Also use `XGBClassifier` from `xgboost` where you may need to include `objective=\"binary:logistic\"` as an option. The default scoring method for all of the `sklearn` classifiers is accuracy. Apply `my_classifier_results` to the test data for each model.\n",
    "\n",
    "* Now use the four hyperparameter optimization techniques on `XGBClassifier` and TPOT general model optimization.  Apply `my_classifer_results` to the test data in each case.\n",
    "    * Feel free to use 3 folds instead of 5 for cross validation to speed things up. \n",
    "    * Choose a very small number of iterations, population size, etc. until you're sure things are working correctly, then turn up the numbers.  General TPOT optimization will take a while (fair warning: it took about 30 minutes on my Macbook Pro with generations = 10, population_size=40, and cv=5)  \n",
    "    * The hyperparameters to consider for are the same as they were in the presentation , but here they are again for convenience:\n",
    "\n",
    "Hyperparameter | Type | Default Value | Typical Range\n",
    "---- | ---- | ---- | ----\n",
    "n_estimators | discrete / integer | 100 | 50 to 150\n",
    "max_depth | discrete / integer | 3| 1 to 10\n",
    "min_child_weight | discrete / integer | 1 | 1 to 20\n",
    "learning_rate | continuous / float | 0.1 | 0.001 to 1\n",
    "sub_sample | continuous / float | 1 | 0.05 to 1\n",
    "reg_lambda | continuous / float | 1 | 0 to 5\n",
    "reg_alpha  | continuous / float | 0 | 0 to 5\n",
    "\n",
    "* Write a summary as described above, but also discuss the following:\n",
    "    * If the bank is primarily interested in correctly identifying loans that are truly bad, then which model should they use?  Why?\n",
    "* Use section headers to label your work.  Your summary / discussion should be more than simply \"XYZ is the best model\", but it also shouldn't be more than a few paragraphs and a table.\n",
    "\n",
    "<font color = \"blue\"> *** 30 points -  add cells below as needed *** (don't delete this cell) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this cell for loading and preparing the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = pd.read_csv('./data/loans_subset.csv')\n",
    "\n",
    "# split into predictors and target\n",
    "# convert to numpy arrays for xgboost, OK for other models too\n",
    "y = np.array(X['status_Bad']) # 1 for bad loan, 0 for good loan\n",
    "X = np.array(X.drop(columns = ['status_Bad']))\n",
    "\n",
    "# split into test and training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5475\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# we do need to go higher than the default iterations for the solver to get convergence\n",
    "# and the explicity declaration of the solver avoids a warning message, otherwise\n",
    "# the parameters are defaults.\n",
    "logreg_model = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Use score method to get accuracy of model\n",
    "score = logreg_model.score(X_test, y_test) # this is accuracy\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred:bad</th>\n",
       "      <th>pred:good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true: bad</th>\n",
       "      <td>126</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true:good</th>\n",
       "      <td>110</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred:bad  pred:good\n",
       "true: bad       126         71\n",
       "true:good       110         93"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtaining the confusion matrix and making it look nice\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# must put true before predictions in confusion matrix function\n",
    "cmtx = pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred, labels=[1,0]), \n",
    "    index=['true:bad', 'true:good'], \n",
    "    columns=['pred:bad','pred:good']\n",
    ")\n",
    "display(cmtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
